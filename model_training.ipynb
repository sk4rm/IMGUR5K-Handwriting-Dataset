{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-26T12:44:55.952429Z",
     "start_time": "2024-08-26T12:44:53.327426Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import cv2\n",
    "import torch\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\" # I use AMD... ðŸ˜­\n",
    "device"
   ],
   "id": "initial_id",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Initialize dataset directory",
   "id": "cb24d90f21c08e10"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-26T12:44:55.956777Z",
     "start_time": "2024-08-26T12:44:55.953795Z"
    }
   },
   "cell_type": "code",
   "source": [
    "train_dir = ''\n",
    "test_dir = '' \n",
    "val_dir = ''"
   ],
   "id": "5e01ec44c313d9c8",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Initialize dataset",
   "id": "3908f1236557f8f8"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "One of the data necessary is the content of the image. To achieve this, we will have to generate an image with plain white background, using Verily Serif Mono font.",
   "id": "b08d4f51bbdada08"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-26T12:44:56.002930Z",
     "start_time": "2024-08-26T12:44:55.956777Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from PIL import Image, ImageDraw, ImageFont\n",
    "def draw_word(word: str) -> Image:\n",
    "    text_len = len(word)\n",
    "    font_size = 50\n",
    "    w = max(256, int(text_len * font_size * 0.64))\n",
    "    h = 256\n",
    "    \n",
    "    img = Image.new('RGB', (w,h), color=(255, 255, 255))\n",
    "    font = ImageFont.truetype('VerilySerifMono.otf', font_size)\n",
    "    d = ImageDraw.Draw(img)\n",
    "    text_width = d.textlength(word, font)\n",
    "    position = ((w - text_width) / 2, (h-font_size) / 2)\n",
    "    \n",
    "    d.text(position, word, font=font, fill=0)\n",
    "    return img"
   ],
   "id": "e0f89d2c3e51d128",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-26T12:44:57.529295Z",
     "start_time": "2024-08-26T12:44:56.003936Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import random\n",
    "\n",
    "class ImageDataset(Dataset):\n",
    "    def __init__(self, image_dir, word_dir):\n",
    "        self.image_dir = Path(image_dir)\n",
    "        with open(word_dir, 'r') as f:\n",
    "            self.words = json.load(f)\n",
    "        self.images = list(self.image_dir.glob('*.jpg'))\n",
    "        self.transform = transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Resize((256, 256))\n",
    "        ])\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        assert idx <= len(self), 'Index out of range'\n",
    "        try:\n",
    "            rgb_img = self.transform(Image.open(self.images[idx]).convert('RGB'))\n",
    "            \n",
    "            content = random.choice(list(self.words.values()))\n",
    "            allowed_symbols = '0123456789abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ!\\\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'\n",
    "            content = ''.join([i for i in content if i in allowed_symbols])\n",
    "            while not content:\n",
    "                content = random.choice(list(self.words.values()))\n",
    "                content = ''.join([i for i in content if i in allowed_symbols])\n",
    "            img_content = self.transform(draw_word(content))\n",
    "            \n",
    "            content_style = self.words[self.images[idx].stem]\n",
    "            content_style = ''.join([i for i in content_style if i in allowed_symbols])\n",
    "            if not content_style:\n",
    "                content_style = 'o'\n",
    "            img_content_style = self.transform(draw_word(content_style))\n",
    "            return rgb_img, img_content, content, img_content_style, content_style\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            return torch.tensor(-1), torch.tensor(-1)\n",
    "    \n",
    "        # item = {'image': img, 'idx': idx, 'label': self.words[self.images[idx].name]}\n",
    "        # return item"
   ],
   "id": "362acd3572fd2161",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Loss function",
   "id": "582dd2ca84b52d59"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 1. VGG19 Typeface classifier, C - Text Perceptual Loss\n",
    "- Perceptual loss computed from the feature maps at layer i denoted as Ï†i and Mi is the number of elements in the particular feature map which is used as normalization. Only computed for output image corresponding to original content.\n",
    "- Texture loss / style loss computed from Gram matrix of the feature maps.\n",
    "- Embedding-based loss computed from feature maps of the penultimate layer of this network. (???)\n",
    "\n",
    "https://gist.github.com/alper111/8233cdb0414b4cb5853f2f730ab95a49#gistcomment-3347450"
   ],
   "id": "9ee389dd203780c2"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-26T12:44:57.538753Z",
     "start_time": "2024-08-26T12:44:57.530884Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torchvision\n",
    "# Paper trains this model with Synth-Font dataset, however I could not find it\n",
    "class VGGPerceptualLoss(torch.nn.Module):\n",
    "    def __init__(self, resize=True):\n",
    "        super(VGGPerceptualLoss, self).__init__()\n",
    "        blocks= [torchvision.models.vgg19(pretrained=True).features[:4].eval(),\n",
    "                 torchvision.models.vgg19(pretrained=True).features[4:9].eval(),\n",
    "                 torchvision.models.vgg19(pretrained=True).features[9:16].eval(),\n",
    "                 torchvision.models.vgg19(pretrained=True).features[16:23].eval()]\n",
    "        for bl in blocks:\n",
    "            for p in bl.parameters():   \n",
    "                p.requires_grad = False\n",
    "        self.blocks = torch.nn.ModuleList(blocks)\n",
    "        self.transform = torch.nn.functional.interpolate\n",
    "        self.resize = resize\n",
    "        self.register_buffer('mean', torch.tensor([0.485, 0.456, 0.406]).view(1, 3, 1, 1))\n",
    "        self.register_buffer('std', torch.tensor([0.229, 0.224, 0.225]).view(1, 3, 1, 1))\n",
    "    \n",
    "    # default: feature_layers=[0, 1, 2, 3], style_layers=[]    \n",
    "    def forward(self, prediction, target, feature_layers=None, style_layers=None):    \n",
    "        if style_layers is None:\n",
    "            style_layers = [0, 1, 2, 3]\n",
    "        if feature_layers is None:\n",
    "            feature_layers = [2]\n",
    "        if prediction.shape[1] != 3:\n",
    "            prediction = prediction.repeat(1, 3, 1, 1)\n",
    "            target = target.repeat(1, 3, 1, 1)\n",
    "        prediction = (prediction - self.mean) / self.std\n",
    "        target = (target-self.mean)/self.std\n",
    "        if self.resize:\n",
    "            prediction = self.transform(prediction, mode='bilinear', size=(224, 224), align_corners=False)\n",
    "            target = self.transform(target, mode='bilinear', size=(224, 224), align_corners=False)\n",
    "        perc_loss = 0.0\n",
    "        gram_loss = 0.0\n",
    "        x = prediction\n",
    "        y = target\n",
    "        for i, block in enumerate(self.blocks):\n",
    "            x = block(x)\n",
    "            y = block(y)\n",
    "            if i in feature_layers:\n",
    "                perc_loss += torch.nn.functional.l1_loss(x, y)\n",
    "            if i in style_layers:\n",
    "                act_x = x.reshape(x.shape[0], x.shape[1], -1)\n",
    "                act_y = y.reshape(y.shape[0], y.shape[1], -1)\n",
    "                gram_x = act_x @ act_x.permute(0, 2, 1)\n",
    "                gram_y = act_y @ act_y.permute(0, 2, 1)\n",
    "                gram_loss += torch.nn.functional.l1_loss(gram_x, gram_y)\n",
    "        return perc_loss, gram_loss"
   ],
   "id": "283cb2611445278",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 2. OCR, R - Text Content Loss\n",
    "- The relevant modules has already been added along with the saved configuration mentioned in the paper, and the PyTorch model can be downloaded [here](https://drive.google.com/file/d/1b59rXuGGmKne1AuHnkgDzoYgKeETNMv9/view?usp=drive_link). Have not implemented it to calculate loss yet.\n",
    "- The content loss is computed by measuring the cross entropy between the sequence of characters in the input string, c1, c2, the predicted string, c'1, c'2 respectively and are represented as one-hot vectors.\n",
    "\n",
    "https://github.com/clovaai/deep-text-recognition-benchmark"
   ],
   "id": "dd0b230c5ac13059"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-26T12:44:57.549706Z",
     "start_time": "2024-08-26T12:44:57.540243Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from torch import nn\n",
    "from ocr.STRFL import TRBA, Options\n",
    "from torchvision import transforms as T\n",
    "from PIL import Image\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class resizeNormalize():\n",
    "    def __init__(self, size, interpolation=Image.BILINEAR):\n",
    "        self.size = size\n",
    "        self.interpolation = interpolation\n",
    "        self.toTensor = T.ToTensor()\n",
    "\n",
    "    def __call__(self, batch):\n",
    "        batch = T.Resize(self.size)(batch)\n",
    "        for n in range(batch.shape[0]):\n",
    "            batch[n].sub_(0.5).div_(0.5)\n",
    "        return batch\n",
    "\n",
    "class OCRLoss(nn.Module):\n",
    "    def __init__(self,\n",
    "                 model_remote_path: str = 'ocr/TRBA-PR.pth',\n",
    "                 model_local_path: str = 'ocr/TRBA-PR.pth'):\n",
    "        super().__init__()\n",
    "\n",
    "        img_h = 32\n",
    "        img_w = 100\n",
    "\n",
    "        opt = Options()\n",
    "        self.opt = opt\n",
    "        model = nn.DataParallel(TRBA(opt))\n",
    "        model.load_state_dict(torch.load(model_local_path))\n",
    "        self.model = model.module\n",
    "\n",
    "        self.transform = resizeNormalize((img_h, img_w))\n",
    "\n",
    "        self.converter = opt.Converter\n",
    "        self.criterion = torch.nn.CrossEntropyLoss(ignore_index=opt.Converter.dict[\"[PAD]\"])\n",
    "\n",
    "        self.model.train()\n",
    "\n",
    "    def forward(self, images, labels, return_recognized=False):\n",
    "        batch_size = images.size(0)\n",
    "        # For max length prediction\n",
    "        text_for_pred = (\n",
    "            torch.LongTensor(batch_size)\n",
    "            .fill_(self.opt.Converter.dict[\"[SOS]\"])\n",
    "            .cuda()\n",
    "        )\n",
    "        labels_index, labels_length = self.opt.Converter.encode(\n",
    "            labels, batch_max_length=25\n",
    "        )\n",
    "        target = labels_index[:, 1:]  # without [SOS] Symbol\n",
    "\n",
    "        preds = self.model(self.transform(images), text_for_pred, is_train=False)\n",
    "        loss = self.criterion(\n",
    "            preds.view(-1, preds.shape[-1]), target.contiguous().view(-1)\n",
    "        )\n",
    "\n",
    "        if return_recognized:\n",
    "            preds_size = torch.IntTensor([preds.size(1)] * batch_size).cuda()\n",
    "            _, preds_index = preds.max(2)\n",
    "            preds_str = self.opt.Converter.decode(preds_index, preds_size)\n",
    "            preds_prob = F.softmax(preds, dim=2)\n",
    "            preds_max_prob, _ = preds_prob.max(dim=2)\n",
    "\n",
    "            recognized = []\n",
    "\n",
    "            for pred, pred_max_prob in zip(preds_str, preds_max_prob):\n",
    "                pred_EOS = pred.find(\"[EOS]\")\n",
    "                pred = pred[:pred_EOS]  # prune after \"end of sentence\" token ([s])\n",
    "                pred_max_prob = pred_max_prob[:pred_EOS]\n",
    "                recognized.append(pred)\n",
    "\n",
    "            return loss, recognized\n",
    "        return loss"
   ],
   "id": "9d2de7de668bacef",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-26T12:44:57.554458Z",
     "start_time": "2024-08-26T12:44:57.551148Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# from torch import nn\n",
    "# from ocr.utils import AttnLabelConverter\n",
    "# from ocr.model import Model\n",
    "# \n",
    "# class opt:\n",
    "#     # Hardcoding the arguments instead of using argument parsers \n",
    "#     # as per https://github.com/clovaai/deep-text-recognition-benchmark/blob/master/demo.py#L96\n",
    "#     image_folder = 'images'\n",
    "#     workers = 4\n",
    "#     batch_size = 192\n",
    "#     saved_model = 'ocr/TPS-ResNet-BiLSTM-Attn.pth'\n",
    "#     batch_max_length = 25\n",
    "#     imgH = 32\n",
    "#     imgW = 100\n",
    "#     rgb = False # See input_channel comment below\n",
    "#     character = '0123456789abcdefghijklmnopqrstuvwxyz'\n",
    "#     sensitive = False\n",
    "#     PAD = False\n",
    "#     Transformation = 'TPS'\n",
    "#     FeatureExtraction = 'ResNet'\n",
    "#     SequenceModeling = 'BiLSTM'\n",
    "#     Prediction = 'Attn'\n",
    "#     num_fiducial = 20\n",
    "#     input_channel = 1\n",
    "#     output_channel = 512\n",
    "#     hidden_size = 256\n",
    "#     num_class = 0\n",
    "#     \n",
    "# class OCRLoss(nn.Module):\n",
    "#     def __init__(self):\n",
    "#         super(OCRLoss, self).__init__()\n",
    "#         \n",
    "#         self.converter = AttnLabelConverter(opt.character)\n",
    "#         self.opt = opt()\n",
    "#         self.opt.num_class = len(self.converter.character)\n",
    "#         \n",
    "#         if self.opt.rgb:\n",
    "#             self.opt.input_channel = 3 # This breaks loading, input_channel has to be 1, or rgb false\n",
    "#         \n",
    "#         self.model = Model(self.opt)\n",
    "#         self.model = torch.nn.DataParallel(self.model).to(device)\n",
    "#         \n",
    "#         mappings = torch.load(opt.saved_model, map_location=device)\n",
    "#         self.model.load_state_dict(mappings)\n",
    "#         self.model.eval()\n",
    "#         self.criterion = nn.CrossEntropyLoss(ignore_index=True) # Unsure what setting for this\n",
    "#         \n",
    "#         \n",
    "#     def forward(self, image, label):\n",
    "#         batch_size = image.size(0)\n",
    "#         text = torch.LongTensor(batch_size, opt.batch_max_length +1).fill_(0).to(device)\n",
    "#         image = transforms.Grayscale(1)(image)\n",
    "#         print(label)\n",
    "#         labels_index, labels_length = self.converter.encode(label, batch_max_length=25)\n",
    "#         target = labels_index[:, 1:]\n",
    "#         preds = self.model(image, text)\n",
    "#         loss = self.criterion(\n",
    "#             preds.view(-1, preds.shape[-1]), \n",
    "#             target.contiguous().view(-1)\n",
    "#         )\n",
    "#         \n",
    "#         return loss"
   ],
   "id": "a8bcda5e268eb161",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# 1. Style Encoder\n",
    "This is a homebrew version of ResNet34 architecture,I have no idea if this works, but I am prepared to alter it in the events it fails. "
   ],
   "id": "d1f723138fb31fd5"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-26T12:44:57.557966Z",
     "start_time": "2024-08-26T12:44:57.555519Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# class ResidualBlock(nn.Module):\n",
    "#     def __init__(self, in_channels, out_channels):\n",
    "#         super(ResidualBlock, self).__init__()\n",
    "#         self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3)\n",
    "#         self.bn1 = nn.BatchNorm2d(out_channels)\n",
    "#         self.relu = nn.ReLU()\n",
    "#         self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3)\n",
    "#         self.bn2 = nn.BatchNorm2d(out_channels)\n",
    "#         \n",
    "#         self.shortcut = nn.Sequential()\n",
    "#         if in_channels != out_channels:\n",
    "#             self.shortcut = nn.Sequential(\n",
    "#                 nn.Conv2d(in_channels, out_channels, kernel_size=1),\n",
    "#                 nn.BatchNorm2d(out_channels)\n",
    "#             )\n",
    "#         \n",
    "#     def forward(self, x):\n",
    "#         out = self.relu(self.bn1(self.conv1(x)))\n",
    "#         out = self.bn2(self.conv2(out))\n",
    "#         out += self.shortcut(x)\n",
    "#         out = self.relu(out)\n",
    "#         return out"
   ],
   "id": "a24a8d6eb648d084",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-26T12:44:57.561476Z",
     "start_time": "2024-08-26T12:44:57.557966Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# from torch import nn\n",
    "# import torchvision\n",
    "# class StyleEncoder(nn.Module):\n",
    "#     def __init__(self):\n",
    "#         super(StyleEncoder, self).__init__()\n",
    "#         self.layer_stack = nn.Sequential(\n",
    "#             nn.Conv2d(in_channels=3, out_channels=32, kernel_size=3), #256x256\n",
    "#             nn.ReLU(inplace=True),\n",
    "#             nn.BatchNorm2d(32),\n",
    "#             nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3),\n",
    "#             nn.ReLU(inplace=True),\n",
    "#             nn.BatchNorm2d(64),\n",
    "#             nn.MaxPool2d(kernel_size=2, stride=2), #128x128\n",
    "#             self._make_layer(ResidualBlock, 64, 128, stride=2),\n",
    "#             nn.Conv2d(in_channels=128, out_channels=128, kernel_size=3),\n",
    "#             nn.ReLU(inplace=True),\n",
    "#             nn.BatchNorm2d(128),\n",
    "#             nn.MaxPool2d(kernel_size=2, stride=2), #64x64\n",
    "#             self._make_layer(ResidualBlock, 128, 256, stride=2),\n",
    "#             nn.Conv2d(in_channels=256, out_channels=256, kernel_size=3),\n",
    "#             nn.ReLU(inplace=True),\n",
    "#             nn.BatchNorm2d(256),\n",
    "#             nn.MaxPool2d(kernel_size=2, stride=2), #32x32\n",
    "#             self._make_layer(ResidualBlock, 256, 512, stride=2),\n",
    "#             nn.Conv2d(in_channels=512, out_channels=512, kernel_size=3),\n",
    "#             nn.ReLU(inplace=True),\n",
    "#             nn.BatchNorm2d(512),\n",
    "#             nn.MaxPool2d(kernel_size=2, stride=2), #16x16\n",
    "#             self._make_layer(ResidualBlock, 512, 512, stride=2),\n",
    "#             nn.Conv2d(in_channels=512, out_channels=512, kernel_size=3, stride=1),\n",
    "#         )\n",
    "#         \n",
    "#     def _make_layer(self, block: ResidualBlock, in_channels, out_channels, num_blocks, stride):\n",
    "#         strides = [stride] + [1]*(num_blocks-1)\n",
    "#         layers = []\n",
    "#         for stride in strides:\n",
    "#             layers.append(block(in_channels, out_channels))\n",
    "#             self.in_channels = out_channels\n",
    "#         return nn.Sequential(*layers)\n",
    "#     \n",
    "#     def forward(self, x):\n",
    "#         return torchvision.ops.roi_align(input=self.layer_stack(x)) #1x1"
   ],
   "id": "8494437a481da690",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 2. Content Encoder",
   "id": "4f8f2ccfcc8fe60"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-26T12:44:57.565096Z",
     "start_time": "2024-08-26T12:44:57.562482Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# from torch import nn\n",
    "# import torchvision\n",
    "# class ContentEncoder(nn.Module):\n",
    "#     def __init__(self):\n",
    "#         super(ContentEncoder, self).__init__()\n",
    "#         self.layer_stack = nn.Sequential(\n",
    "#             nn.Conv2d(in_channels=3, out_channels=32, kernel_size=3), #256x256\n",
    "#             nn.ReLU(inplace=True),\n",
    "#             nn.BatchNorm2d(32),\n",
    "#             nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3),\n",
    "#             nn.ReLU(inplace=True),\n",
    "#             nn.BatchNorm2d(64),\n",
    "#             nn.MaxPool2d(kernel_size=2, stride=2), #128x128\n",
    "#             self._make_layer(self, ResidualBlock, 64, 128, stride=2),\n",
    "#             nn.Conv2d(in_channels=128, out_channels=128, kernel_size=3),\n",
    "#             nn.ReLU(inplace=True),\n",
    "#             nn.BatchNorm2d(128),\n",
    "#             nn.MaxPool2d(kernel_size=2, stride=2), #64x64\n",
    "#             self._make_layer(self, ResidualBlock, 128, 256, stride=2),\n",
    "#             nn.Conv2d(in_channels=256, out_channels=256, kernel_size=3),\n",
    "#             nn.ReLU(inplace=True),\n",
    "#             nn.BatchNorm2d(256),\n",
    "#             nn.MaxPool2d(kernel_size=2, stride=2), #32x32\n",
    "#             self._make_layer(self, ResidualBlock, 256, 512, stride=2),\n",
    "#             nn.Conv2d(in_channels=512, out_channels=512, kernel_size=3),\n",
    "#             nn.ReLU(inplace=True),\n",
    "#             nn.BatchNorm2d(512),\n",
    "#             nn.MaxPool2d(kernel_size=2, stride=2), #16x16\n",
    "#             self._make_layer(self, ResidualBlock, 512, 512, stride=2),\n",
    "#             nn.Conv2d(in_channels=512, out_channels=512, kernel_size=3, stride=1),\n",
    "#         )\n",
    "#         \n",
    "#     def _make_layer(self, block, in_channels, out_channels, num_blocks, stride):\n",
    "#         strides = [stride] + [1]*(num_blocks-1)\n",
    "#         layers = []\n",
    "#         for stride in strides:\n",
    "#             layers.append(block(in_channels, out_channels, stride))\n",
    "#             self.in_channels = out_channels\n",
    "#         return nn.Sequential(*layers)\n",
    "#     \n",
    "#     def forward(self, x):\n",
    "#         return self.layer_stack(x)"
   ],
   "id": "cd1be39e7aa8c18d",
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-26T12:44:57.570048Z",
     "start_time": "2024-08-26T12:44:57.566106Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from torchvision.ops import roi_align\n",
    "from torchvision import models\n",
    "from torchvision.models.resnet import BasicBlock\n",
    "\n",
    "class ContentEncoder(models.ResNet):\n",
    "    def __init__(self):\n",
    "        # resnet18 init\n",
    "        super().__init__(BasicBlock, [2, 2, 2, 2])\n",
    "\n",
    "    def _forward_impl(self, x):\n",
    "        # See note [TorchScript super()]\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.maxpool(x)\n",
    "\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "\n",
    "        #x = self.avgpool(x)\n",
    "        #x = torch.flatten(x, 1)\n",
    "        #x = self.fc(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class StyleEncoder(models.ResNet):\n",
    "    def __init__(self):\n",
    "        # resnet18 init\n",
    "        super().__init__(BasicBlock, [2, 2, 2, 2])\n",
    "        self.fc = torch.nn.Identity()"
   ],
   "id": "49c9449a4fdc6861",
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# 3. Style Mapping Network\n",
    "- Converts style representations to layer-specific style representations which are then fed as AdaIN normalization coefficient to each layer of the generator. \n",
    "-  eliminate the use of the noise vector input of the standard StyleGAN2"
   ],
   "id": "e5de306ee9b9e446"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-26T12:44:57.576908Z",
     "start_time": "2024-08-26T12:44:57.571080Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import functools\n",
    "class NLayerDiscriminator(nn.Module):\n",
    "    \"\"\"Defines a PatchGAN discriminator\"\"\"\n",
    "\n",
    "    def __init__(self, input_nc, ndf=64, n_layers=3, norm_layer=nn.BatchNorm2d):\n",
    "        \"\"\"Construct a PatchGAN discriminator\n",
    "        Parameters:\n",
    "            input_nc (int)  -- the number of channels in input images\n",
    "            ndf (int)       -- the number of filters in the last conv layer\n",
    "            n_layers (int)  -- the number of conv layers in the discriminator\n",
    "            norm_layer      -- normalization layer\n",
    "        \"\"\"\n",
    "        super(NLayerDiscriminator, self).__init__()\n",
    "        if type(norm_layer) == functools.partial:  # no need to use bias as BatchNorm2d has affine parameters\n",
    "            use_bias = norm_layer.func == nn.InstanceNorm2d\n",
    "        else:\n",
    "            use_bias = norm_layer == nn.InstanceNorm2d\n",
    "\n",
    "        kw = 4\n",
    "        padw = 1\n",
    "        sequence = [nn.Conv2d(input_nc, ndf, kernel_size=kw, stride=2, padding=padw), nn.LeakyReLU(0.2, True)]\n",
    "        nf_mult = 1\n",
    "        nf_mult_prev = 1\n",
    "        for n in range(1, n_layers):  # gradually increase the number of filters\n",
    "            nf_mult_prev = nf_mult\n",
    "            nf_mult = min(2 ** n, 8)\n",
    "            sequence += [\n",
    "                nn.Conv2d(ndf * nf_mult_prev, ndf * nf_mult, kernel_size=kw, stride=2, padding=padw, bias=use_bias),\n",
    "                norm_layer(ndf * nf_mult),\n",
    "                nn.LeakyReLU(0.2, True)\n",
    "            ]\n",
    "\n",
    "        nf_mult_prev = nf_mult\n",
    "        nf_mult = min(2 ** n_layers, 8)\n",
    "        sequence += [\n",
    "            nn.Conv2d(ndf * nf_mult_prev, ndf * nf_mult, kernel_size=kw, stride=1, padding=padw, bias=use_bias),\n",
    "            norm_layer(ndf * nf_mult),\n",
    "            nn.LeakyReLU(0.2, True)\n",
    "        ]\n",
    "\n",
    "        sequence += [nn.Conv2d(ndf * nf_mult, 1, kernel_size=kw, stride=1, padding=padw)]  # output 1 channel prediction map\n",
    "        self.model = nn.Sequential(*sequence)\n",
    "\n",
    "    def forward(self, input):\n",
    "        \"\"\"Standard forward.\"\"\"\n",
    "        return self.model(input)"
   ],
   "id": "a3e59f3c3be6e829",
   "outputs": [],
   "execution_count": 12
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Training",
   "id": "3693c2743a2205a8"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-26T12:44:57.589347Z",
     "start_time": "2024-08-26T12:44:57.577913Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "class StyleGanTrainer:\n",
    "    def __init__(self,\n",
    "                 model_G: nn.Module,\n",
    "                 model_D: nn.Module,\n",
    "                 style_encoder: nn.Module,\n",
    "                 content_encoder: nn.Module,\n",
    "                 optimizer_G: torch.optim.Optimizer,\n",
    "                 optimizer_D: torch.optim.Optimizer,\n",
    "                 scheduler_G: torch.optim.lr_scheduler.LRScheduler,\n",
    "                 scheduler_D: torch.optim.lr_scheduler.LRScheduler,\n",
    "                 train_dataloader: torch.utils.data.DataLoader,\n",
    "                 val_dataloader: torch.utils.data.DataLoader,\n",
    "                 total_epochs: int,\n",
    "                 ocr_loss: nn.Module,\n",
    "                 # typeface_loss: nn.Module,\n",
    "                 perc_loss: nn.Module,\n",
    "                 cons_loss: nn.Module,\n",
    "                 adv_loss: nn.Module,\n",
    "                 device: str):\n",
    "        \n",
    "        self.style_encoder = style_encoder\n",
    "        self.content_encoder = content_encoder\n",
    "        self.model_G = model_G\n",
    "        self.model_D = model_D\n",
    "        self.optimizer_G = optimizer_G\n",
    "        self.optimizer_D = optimizer_D\n",
    "        self.scheduler_G = scheduler_G\n",
    "        self.scheduler_D = scheduler_D\n",
    "        self.train_dataloader = train_dataloader\n",
    "        self.val_dataloader = val_dataloader\n",
    "        self.device = device\n",
    "        self.total_epochs = total_epochs\n",
    "        # self.typeface_loss = typeface_loss.to(device)\n",
    "        self.ocr_loss = ocr_loss.to(device)\n",
    "        self.perc_loss = perc_loss.to(device)\n",
    "        self.cons_loss = cons_loss.to(device)\n",
    "        self.adv_loss = adv_loss.to(device)\n",
    "        \n",
    "    def set_requires_grad(self, net: nn.Module, requires_grad: bool=False):\n",
    "        if net is not None:\n",
    "            for param in net.parameters():\n",
    "                param.requires_grad = requires_grad\n",
    "                \n",
    "    def model_D_loss(self, style_img: torch.Tensor, content_encodes: torch.Tensor, style_encodes: torch.Tensor):\n",
    "        pred_D_fake = self.model_D(self.model_G(content_encodes, style_encodes).detach())\n",
    "        pred_D_real = self.model_D(style_img)\n",
    "        fake = torch.tensor(0.).expand_as(pred_D_fake).to(self.device)\n",
    "        real = torch.tensor(1.).expand_as(pred_D_real).to(self.device)\n",
    "        return (self.adv_loss(pred_D_real, real) + self.adv_loss(pred_D_fake, fake)) / 2.\n",
    "    \n",
    "    def model_G_loss(self, preds: torch.Tensor):\n",
    "        pred_D_fake = self.model_D(preds)\n",
    "        valid = torch.tensor(1.).expand_as(pred_D_fake).to(self.device)\n",
    "        return self.adv_loss(pred_D_fake, valid)\n",
    "                \n",
    "    def train(self):\n",
    "        self.model_G.train()\n",
    "        self.model_D.train()\n",
    "        self.style_encoder.train()\n",
    "        self.content_encoder.train()\n",
    "        \n",
    "        for style_img, desired_content, desired_labels, style_content, style_labels in self.train_dataloader:\n",
    "            if max(len(label) for label in desired_labels) > 25:\n",
    "                continue\n",
    "            if max(len(label) for label in style_labels) > 25:\n",
    "                continue\n",
    "            \n",
    "            self.optimizer_G.zero_grad()\n",
    "            self.optimizer_D.zero_grad()\n",
    "            \n",
    "            style_img = style_img.to(self.device)\n",
    "            desired_content = desired_content.to(self.device)\n",
    "            style_content = style_content.to(self.device)\n",
    "            \n",
    "            style_encodes = self.style_encoder(style_img)\n",
    "            content_encodes = self.content_encoder(desired_content)\n",
    "            style_content_encodes = self.content_encoder(style_content)\n",
    "            \n",
    "            ## Calculate D loss\n",
    "            self.set_requires_grad(self.model_D, True)\n",
    "            self.set_requires_grad(self.model_G, False)\n",
    "            loss_D = self.model_D_loss(style_img, style_content_encodes, style_encodes)\n",
    "            \n",
    "            ## Calculate G loss\n",
    "            self.set_requires_grad(self.model_D, False)\n",
    "            self.set_requires_grad(self.model_G, True)\n",
    "            \n",
    "            preds = self.model_G(content_encodes, style_encodes)\n",
    "            ocr_loss = self.ocr_loss(preds, desired_labels)\n",
    "            \n",
    "            reconstructed = self.model_G(style_content_encodes, style_encodes)\n",
    "            reconstructed_loss = self.cons_loss(style_img, reconstructed)\n",
    "            \n",
    "            reconstructed_style_encode = self.style_encoder(reconstructed)\n",
    "            cycled = model_G(style_content_encodes, reconstructed_style_encode)\n",
    "            cycled_loss = self.cons_loss(style_img, cycled)\n",
    "            \n",
    "            ocr_loss_rec = self.ocr_loss(reconstructed, style_labels)\n",
    "            ocr_loss_total = (ocr_loss + ocr_loss_rec) / 2.\n",
    "            \n",
    "            perc_loss, tex_loss = self.perc_loss(style_img, reconstructed)\n",
    "            enc_loss = 0  # self.typeface_loss(style_img, reconstructed)\n",
    "            \n",
    "            adv_loss = self.model_G_loss(reconstructed)\n",
    "            \n",
    "            loss_G = 0.07 * ocr_loss_total + 2.0 * cycled_loss + 2.0 * reconstructed_loss + 25.0 * perc_loss + 7.0 * tex_loss + 0 * enc_loss + 0.06 * adv_loss\n",
    "            \n",
    "            self.set_requires_grad(self.model_D, True)\n",
    "            \n",
    "            loss_G.backward()\n",
    "            loss_D.backward()\n",
    "            \n",
    "            self.optimizer_D.step()\n",
    "            self.optimizer_G.step()\n",
    "            \n",
    "            print(f\"Loss D: {loss_D.item()}, Loss G: {loss_G.item()}\")\n",
    "            \n",
    "    def validate(self, epoch: int):\n",
    "        print(\"Start validating...\")\n",
    "        self.model_G.eval()\n",
    "        self.model_D.eval()\n",
    "        self.style_encoder.eval()\n",
    "        self.content_encoder.eval()\n",
    "        \n",
    "        for style_img, desired_content, desired_labels, style_content, style_labels in self.val_dataloader:\n",
    "            if max(len(label) for label in desired_labels) > 25:\n",
    "                continue\n",
    "            if max(len(label) for label in style_labels) > 25:\n",
    "                continue\n",
    "            \n",
    "            self.optimizer_G.zero_grad()\n",
    "            self.optimizer_D.zero_grad()\n",
    "            \n",
    "            style_img = style_img.to(self.device)\n",
    "            desired_content = desired_content.to(self.device)\n",
    "            style_content = style_content.to(self.device)\n",
    "            style_encodes = self.style_encoder(style_img)\n",
    "            content_encodes = self.content_encoder(desired_content)\n",
    "            \n",
    "            preds = self.model_G(content_encodes, style_encodes)\n",
    "            ocr_loss = self.ocr_loss(preds, desired_labels)\n",
    "            \n",
    "            style_label_encodes = self.content_encoder(style_labels)\n",
    "            \n",
    "            reconstructed = model_G(style_label_encodes, style_encodes)\n",
    "            reconstructed_loss = self.cons_loss(style_img, reconstructed)\n",
    "            \n",
    "            reconstructed_style_encode = self.style_encoder(reconstructed)\n",
    "            cycle = model_G(style_label_encodes, reconstructed_style_encode)\n",
    "            cycle_loss = self.cons_loss(style_img, cycle)\n",
    "            \n",
    "            ocr_loss_rec = self.ocr_loss(reconstructed, style_labels)\n",
    "            ocr_loss_total = (ocr_loss + ocr_loss_rec) / 2.\n",
    "            \n",
    "            perc_loss, tex_loss = self.perc_loss(style_img, preds)\n",
    "            enc_loss = 0 # self.typeface_loss(style_img, preds)\n",
    "            adv_loss = self.model_G_loss(reconstructed)\n",
    "            \n",
    "            loss = 0.07 * ocr_loss_total + 2.0 * cycle_loss + 2.0 * reconstructed_loss + 25.0 * perc_loss + 7.0 * tex_loss + 0 * enc_loss + 0.06 * adv_loss\n",
    "                \n",
    "    def run(self):\n",
    "        print(self.total_epochs)\n",
    "        for epoch in range(self.total_epochs):\n",
    "            print(f\"Epoch {epoch}:\")\n",
    "            self.train()\n",
    "            with torch.no_grad():\n",
    "                self.validate(epoch)\n",
    "            if self.scheduler_G is not None:\n",
    "                self.scheduler_G.step()\n",
    "            if self.scheduler_D is not None:\n",
    "                self.scheduler_D.step()"
   ],
   "id": "ff8d4f70838fd74b",
   "outputs": [],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-26T13:21:12.300752Z",
     "start_time": "2024-08-26T12:44:57.590851Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from stylegan import StyleBased_Generator\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "total_epochs = 500\n",
    "\n",
    "batch_size = 4\n",
    "train_dataloader = DataLoader(ImageDataset(train_dir, train_dir+'/words.json'), shuffle=True, batch_size=batch_size)\n",
    "val_dataloader = DataLoader(ImageDataset(val_dir, val_dir+'/words.json'), batch_size=batch_size)\n",
    "\n",
    "model_G = StyleBased_Generator(dim_latent=512)\n",
    "model_G.to(device)\n",
    "style_encoder = StyleEncoder().to(device)\n",
    "content_encoder = ContentEncoder().to(device)\n",
    "model_D = NLayerDiscriminator(input_nc=3, ndf=64, n_layers=3, norm_layer=(lambda x : torch.nn.Identity()))\n",
    "model_D.to(device)\n",
    "\n",
    "optimizer_G = torch.optim.AdamW(\n",
    "    list(model_G.parameters()) + \n",
    "    list(style_encoder.parameters()) +\n",
    "    list(style_encoder.parameters()),\n",
    "    lr=1e-3,\n",
    "    weight_decay=1e-6\n",
    ")\n",
    "scheduler_G = torch.optim.lr_scheduler.ExponentialLR(\n",
    "    optimizer_G,\n",
    "    gamma=0.9\n",
    ")\n",
    "optimizer_D = torch.optim.AdamW(model_D.parameters(), lr=1e-4)\n",
    "scheduler_D = torch.optim.lr_scheduler.ExponentialLR(\n",
    "    optimizer_D,\n",
    "    gamma=0.9\n",
    ")\n",
    "\n",
    "trainer = StyleGanTrainer(  \n",
    "    model_G,\n",
    "    model_D,\n",
    "    style_encoder,\n",
    "    content_encoder,\n",
    "    optimizer_G,\n",
    "    optimizer_D,\n",
    "    scheduler_G,\n",
    "    scheduler_D,\n",
    "    train_dataloader,\n",
    "    val_dataloader,\n",
    "    total_epochs,\n",
    "    ocr_loss=OCRLoss(),\n",
    "    perc_loss=VGGPerceptualLoss(),\n",
    "    cons_loss=torch.nn.L1Loss(),\n",
    "    adv_loss=torch.nn.MSELoss(),\n",
    "    device=device,\n",
    ")\n",
    "\n",
    "trainer.run()"
   ],
   "id": "67426edacbea3aac",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ryan Chin\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch\\_compile.py:24: UserWarning: optimizer contains a parameter group with duplicate parameters; in future, this will cause an error; see github.com/pytorch/pytorch/issues/40967 for more information\n",
      "  return torch._dynamo.disable(fn, recursive)(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of tokens and characters: 99\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ryan Chin\\AppData\\Roaming\\Python\\Python312\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Ryan Chin\\AppData\\Roaming\\Python\\Python312\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG19_Weights.IMAGENET1K_V1`. You can also use `weights=VGG19_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500\n",
      "Epoch 0:\n",
      "Loss D: 0.5009117722511292, Loss G: 17501450.0\n",
      "Loss D: 0.3885807991027832, Loss G: 67173280.0\n",
      "Loss D: 0.3951806426048279, Loss G: 61550920.0\n",
      "Loss D: 0.3205571174621582, Loss G: 12331708.0\n",
      "Loss D: 0.32435983419418335, Loss G: 10504441.0\n",
      "Loss D: 0.2444569170475006, Loss G: 9152262.0\n",
      "Loss D: 0.2210187315940857, Loss G: 6772652.0\n",
      "Loss D: 0.21752667427062988, Loss G: 6729622.5\n",
      "Loss D: 0.21989993751049042, Loss G: 7030777.0\n",
      "Loss D: 0.2419198751449585, Loss G: 4819789.0\n",
      "Loss D: 0.1972331702709198, Loss G: 4234455.5\n",
      "Loss D: 0.22220565378665924, Loss G: 3913996.0\n",
      "Loss D: 0.2137477993965149, Loss G: 3116867.25\n",
      "Loss D: 0.13397616147994995, Loss G: 3095902.0\n",
      "Loss D: 0.1100049838423729, Loss G: 2557620.0\n",
      "Loss D: 0.11273956298828125, Loss G: 2153620.5\n",
      "Loss D: 0.07969056814908981, Loss G: 2553236.5\n",
      "Loss D: 0.09889070689678192, Loss G: 1905649.625\n",
      "Loss D: 0.1025988906621933, Loss G: 1732439.25\n",
      "Loss D: 0.193779855966568, Loss G: 1500526.75\n",
      "Loss D: 0.07596688717603683, Loss G: 1504213.75\n",
      "Loss D: 0.07582911849021912, Loss G: 2007471.0\n",
      "Loss D: 0.16885633766651154, Loss G: 1241348.25\n",
      "Loss D: 0.08933985233306885, Loss G: 1064820.0\n",
      "Loss D: 0.13398227095603943, Loss G: 1361981.125\n",
      "Loss D: 0.1675504744052887, Loss G: 1008839.4375\n",
      "Loss D: 0.08564184606075287, Loss G: 1544310.375\n",
      "Loss D: 0.10338776558637619, Loss G: 974240.625\n",
      "Loss D: 0.11002112925052643, Loss G: 1100881.25\n",
      "Loss D: 0.060523368418216705, Loss G: 1300139.375\n",
      "Loss D: 0.07949163764715195, Loss G: 824322.9375\n",
      "Loss D: 0.12901490926742554, Loss G: 735906.0625\n",
      "Loss D: 0.08727429807186127, Loss G: 740731.0\n",
      "Loss D: 0.07558774948120117, Loss G: 715397.6875\n",
      "Loss D: 0.08232233673334122, Loss G: 701864.6875\n",
      "Loss D: 0.0766436755657196, Loss G: 618636.5\n",
      "Loss D: 0.07377257943153381, Loss G: 660562.4375\n",
      "Loss D: 0.06663893163204193, Loss G: 647121.125\n",
      "Loss D: 0.07468273490667343, Loss G: 575156.875\n",
      "Loss D: 0.10180636495351791, Loss G: 556484.5625\n",
      "Loss D: 0.12573619186878204, Loss G: 558781.5\n",
      "Loss D: 0.07210967689752579, Loss G: 652953.0625\n",
      "Loss D: 0.12616170942783356, Loss G: 520450.71875\n",
      "Loss D: 0.11186876893043518, Loss G: 642985.4375\n",
      "Loss D: 0.11995819211006165, Loss G: 398339.875\n",
      "Loss D: 0.12115883827209473, Loss G: 406573.40625\n",
      "Loss D: 0.10101819038391113, Loss G: 505181.78125\n",
      "Loss D: 0.0698162242770195, Loss G: 704149.125\n",
      "Loss D: 0.13095977902412415, Loss G: 371777.9375\n",
      "Loss D: 0.08557520806789398, Loss G: 491376.75\n",
      "Loss D: 0.0950627252459526, Loss G: 417657.75\n",
      "Loss D: 0.09525194019079208, Loss G: 403131.65625\n",
      "Loss D: 0.10096396505832672, Loss G: 359769.8125\n",
      "Loss D: 0.15046659111976624, Loss G: 416659.96875\n",
      "Loss D: 0.059154242277145386, Loss G: 658153.75\n",
      "Loss D: 0.08401672542095184, Loss G: 360707.25\n",
      "Loss D: 0.14336137473583221, Loss G: 393614.5\n",
      "Loss D: 0.11556968837976456, Loss G: 319643.3125\n",
      "Loss D: 0.0855419784784317, Loss G: 618145.625\n",
      "Loss D: 0.15953786671161652, Loss G: 302977.40625\n",
      "Loss D: 0.11884842067956924, Loss G: 310306.71875\n",
      "Loss D: 0.16820234060287476, Loss G: 359692.1875\n",
      "Loss D: 0.08778418600559235, Loss G: 272466.84375\n",
      "Loss D: 0.1686326116323471, Loss G: 295001.65625\n",
      "Loss D: 0.10465867817401886, Loss G: 281532.15625\n",
      "Loss D: 0.0685444250702858, Loss G: 254942.015625\n",
      "Loss D: 0.11069563031196594, Loss G: 549063.5\n",
      "Loss D: 0.07641036063432693, Loss G: 226004.875\n",
      "Loss D: 0.05802454799413681, Loss G: 411452.53125\n",
      "Loss D: 0.10273483395576477, Loss G: 334528.21875\n",
      "Loss D: 0.08654168248176575, Loss G: 271220.0625\n",
      "Loss D: 0.06318160891532898, Loss G: 278395.3125\n",
      "Loss D: 0.12627357244491577, Loss G: 328188.5\n",
      "Loss D: 0.07965158671140671, Loss G: 379147.1875\n",
      "Loss D: 0.08162233978509903, Loss G: 259330.3125\n",
      "Loss D: 0.12050871551036835, Loss G: 305471.625\n",
      "Loss D: 0.13997629284858704, Loss G: 233248.359375\n",
      "Loss D: 0.058019861578941345, Loss G: 279932.84375\n",
      "Loss D: 0.061892349272966385, Loss G: 273714.15625\n",
      "Loss D: 0.03755681589245796, Loss G: 256514.796875\n",
      "Loss D: 0.057722099125385284, Loss G: 250881.65625\n",
      "Loss D: 0.04565431550145149, Loss G: 361753.125\n",
      "Loss D: 0.0799083411693573, Loss G: 196670.84375\n",
      "Loss D: 0.12636038661003113, Loss G: 513601.09375\n",
      "Loss D: 0.0636352151632309, Loss G: 411872.84375\n",
      "Loss D: 0.10851031541824341, Loss G: 368332.6875\n",
      "Loss D: 0.16063916683197021, Loss G: 223407.15625\n",
      "Loss D: 0.13557708263397217, Loss G: 238661.125\n",
      "Loss D: 0.07385248690843582, Loss G: 500859.375\n",
      "Loss D: 0.07184839248657227, Loss G: 246456.828125\n",
      "Loss D: 0.23427987098693848, Loss G: 225910.359375\n",
      "Loss D: 0.11323417723178864, Loss G: 214921.34375\n",
      "Loss D: 0.0921543538570404, Loss G: 284325.84375\n",
      "Loss D: 0.1611926555633545, Loss G: 300933.625\n",
      "Loss D: 0.11608254909515381, Loss G: 246887.265625\n",
      "Loss D: 0.0384838841855526, Loss G: 237113.015625\n",
      "Loss D: 0.04481548070907593, Loss G: 312885.71875\n",
      "Loss D: 0.09603650867938995, Loss G: 168459.890625\n",
      "Loss D: 0.058756060898303986, Loss G: 253561.9375\n",
      "Loss D: 0.11539681255817413, Loss G: 265732.9375\n",
      "Loss D: 0.16480214893817902, Loss G: 288558.625\n",
      "Loss D: 0.10727715492248535, Loss G: 283396.5625\n",
      "Loss D: 0.1321263611316681, Loss G: 218738.078125\n",
      "Loss D: 0.1721530556678772, Loss G: 168640.234375\n",
      "Loss D: 0.16403669118881226, Loss G: 185583.28125\n",
      "Loss D: 0.10739853978157043, Loss G: 178065.734375\n",
      "Loss D: 0.18757441639900208, Loss G: 314943.21875\n",
      "Loss D: 0.08156270533800125, Loss G: 138680.390625\n",
      "Loss D: 0.08681879937648773, Loss G: 179490.65625\n",
      "Loss D: 0.13587629795074463, Loss G: 154544.484375\n",
      "Loss D: 0.13853171467781067, Loss G: 192367.1875\n",
      "Loss D: 0.09732723236083984, Loss G: 211304.59375\n",
      "Loss D: 0.15095634758472443, Loss G: 146779.484375\n",
      "Loss D: 0.23836421966552734, Loss G: 168646.59375\n",
      "Loss D: 0.15002824366092682, Loss G: 142431.625\n",
      "Loss D: 0.14734619855880737, Loss G: 165478.453125\n",
      "Loss D: 0.07278954237699509, Loss G: 281822.90625\n",
      "Loss D: 0.1083652675151825, Loss G: 214006.3125\n",
      "Loss D: 0.2593749761581421, Loss G: 322832.3125\n",
      "Loss D: 0.17118126153945923, Loss G: 163258.25\n",
      "Loss D: 0.10200025141239166, Loss G: 216276.84375\n",
      "Loss D: 0.11364617943763733, Loss G: 233502.40625\n",
      "Loss D: 0.10256051272153854, Loss G: 172913.40625\n",
      "Loss D: 0.23119761049747467, Loss G: 246160.984375\n",
      "Loss D: 0.11515383422374725, Loss G: 159710.15625\n",
      "Loss D: 0.1732814460992813, Loss G: 139591.578125\n",
      "Loss D: 0.19289430975914001, Loss G: 197371.734375\n",
      "Loss D: 0.12668560445308685, Loss G: 265160.8125\n",
      "Loss D: 0.1192135438323021, Loss G: 323190.125\n",
      "Loss D: 0.09852823615074158, Loss G: 277915.28125\n",
      "Loss D: 0.07497262954711914, Loss G: 199460.78125\n",
      "Loss D: 0.13475163280963898, Loss G: 180053.578125\n",
      "Loss D: 0.10396461933851242, Loss G: 171879.703125\n",
      "Loss D: 0.09834371507167816, Loss G: 146697.828125\n",
      "Loss D: 0.13441351056098938, Loss G: 119246.390625\n",
      "Loss D: 0.1307239532470703, Loss G: 158205.203125\n",
      "Loss D: 0.08235974609851837, Loss G: 183471.25\n",
      "Loss D: 0.10805974900722504, Loss G: 182586.71875\n",
      "Loss D: 0.06334220618009567, Loss G: 126366.484375\n",
      "Loss D: 0.12237442284822464, Loss G: 167461.5\n",
      "Loss D: 0.06147836521267891, Loss G: 215783.609375\n",
      "Loss D: 0.14095021784305573, Loss G: 148051.25\n",
      "Loss D: 0.13852275907993317, Loss G: 199693.890625\n",
      "Loss D: 0.1306421011686325, Loss G: 109061.1171875\n",
      "Loss D: 0.169154554605484, Loss G: 123702.1015625\n",
      "Loss D: 0.08959739655256271, Loss G: 203049.09375\n",
      "Loss D: 0.13405632972717285, Loss G: 148319.875\n",
      "Loss D: 0.07269316911697388, Loss G: 171410.0625\n",
      "Loss D: 0.10178635269403458, Loss G: 231124.859375\n",
      "Loss D: 0.10799378156661987, Loss G: 203972.0\n",
      "Loss D: 0.06453631818294525, Loss G: 168394.59375\n",
      "Loss D: 0.08742126822471619, Loss G: 141667.515625\n",
      "Loss D: 0.06204501539468765, Loss G: 148112.734375\n",
      "Loss D: 0.10816892236471176, Loss G: 131131.390625\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[14], line 53\u001B[0m\n\u001B[0;32m     29\u001B[0m scheduler_D \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39moptim\u001B[38;5;241m.\u001B[39mlr_scheduler\u001B[38;5;241m.\u001B[39mExponentialLR(\n\u001B[0;32m     30\u001B[0m     optimizer_D,\n\u001B[0;32m     31\u001B[0m     gamma\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0.9\u001B[39m\n\u001B[0;32m     32\u001B[0m )\n\u001B[0;32m     34\u001B[0m trainer \u001B[38;5;241m=\u001B[39m StyleGanTrainer(  \n\u001B[0;32m     35\u001B[0m     model_G,\n\u001B[0;32m     36\u001B[0m     model_D,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m     50\u001B[0m     device\u001B[38;5;241m=\u001B[39mdevice,\n\u001B[0;32m     51\u001B[0m )\n\u001B[1;32m---> 53\u001B[0m \u001B[43mtrainer\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrun\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "Cell \u001B[1;32mIn[13], line 165\u001B[0m, in \u001B[0;36mStyleGanTrainer.run\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    163\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m epoch \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtotal_epochs):\n\u001B[0;32m    164\u001B[0m     \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mEpoch \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mepoch\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m:\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m--> 165\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtrain\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    166\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m torch\u001B[38;5;241m.\u001B[39mno_grad():\n\u001B[0;32m    167\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mvalidate(epoch)\n",
      "Cell \u001B[1;32mIn[13], line 110\u001B[0m, in \u001B[0;36mStyleGanTrainer.train\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    106\u001B[0m loss_G \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m0.07\u001B[39m \u001B[38;5;241m*\u001B[39m ocr_loss_total \u001B[38;5;241m+\u001B[39m \u001B[38;5;241m2.0\u001B[39m \u001B[38;5;241m*\u001B[39m cycled_loss \u001B[38;5;241m+\u001B[39m \u001B[38;5;241m2.0\u001B[39m \u001B[38;5;241m*\u001B[39m reconstructed_loss \u001B[38;5;241m+\u001B[39m \u001B[38;5;241m25.0\u001B[39m \u001B[38;5;241m*\u001B[39m perc_loss \u001B[38;5;241m+\u001B[39m \u001B[38;5;241m7.0\u001B[39m \u001B[38;5;241m*\u001B[39m tex_loss \u001B[38;5;241m+\u001B[39m \u001B[38;5;241m0\u001B[39m \u001B[38;5;241m*\u001B[39m enc_loss \u001B[38;5;241m+\u001B[39m \u001B[38;5;241m0.06\u001B[39m \u001B[38;5;241m*\u001B[39m adv_loss\n\u001B[0;32m    108\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mset_requires_grad(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmodel_D, \u001B[38;5;28;01mTrue\u001B[39;00m)\n\u001B[1;32m--> 110\u001B[0m \u001B[43mloss_G\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbackward\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    111\u001B[0m loss_D\u001B[38;5;241m.\u001B[39mbackward()\n\u001B[0;32m    113\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39moptimizer_D\u001B[38;5;241m.\u001B[39mstep()\n",
      "File \u001B[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch\\_tensor.py:522\u001B[0m, in \u001B[0;36mTensor.backward\u001B[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001B[0m\n\u001B[0;32m    512\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m has_torch_function_unary(\u001B[38;5;28mself\u001B[39m):\n\u001B[0;32m    513\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m handle_torch_function(\n\u001B[0;32m    514\u001B[0m         Tensor\u001B[38;5;241m.\u001B[39mbackward,\n\u001B[0;32m    515\u001B[0m         (\u001B[38;5;28mself\u001B[39m,),\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    520\u001B[0m         inputs\u001B[38;5;241m=\u001B[39minputs,\n\u001B[0;32m    521\u001B[0m     )\n\u001B[1;32m--> 522\u001B[0m \u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mautograd\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbackward\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    523\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mgradient\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mretain_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcreate_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minputs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43minputs\u001B[49m\n\u001B[0;32m    524\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch\\autograd\\__init__.py:266\u001B[0m, in \u001B[0;36mbackward\u001B[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001B[0m\n\u001B[0;32m    261\u001B[0m     retain_graph \u001B[38;5;241m=\u001B[39m create_graph\n\u001B[0;32m    263\u001B[0m \u001B[38;5;66;03m# The reason we repeat the same comment below is that\u001B[39;00m\n\u001B[0;32m    264\u001B[0m \u001B[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001B[39;00m\n\u001B[0;32m    265\u001B[0m \u001B[38;5;66;03m# calls in the traceback and some print out the last line\u001B[39;00m\n\u001B[1;32m--> 266\u001B[0m \u001B[43mVariable\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_execution_engine\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrun_backward\u001B[49m\u001B[43m(\u001B[49m\u001B[43m  \u001B[49m\u001B[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001B[39;49;00m\n\u001B[0;32m    267\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtensors\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    268\u001B[0m \u001B[43m    \u001B[49m\u001B[43mgrad_tensors_\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    269\u001B[0m \u001B[43m    \u001B[49m\u001B[43mretain_graph\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    270\u001B[0m \u001B[43m    \u001B[49m\u001B[43mcreate_graph\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    271\u001B[0m \u001B[43m    \u001B[49m\u001B[43minputs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    272\u001B[0m \u001B[43m    \u001B[49m\u001B[43mallow_unreachable\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[0;32m    273\u001B[0m \u001B[43m    \u001B[49m\u001B[43maccumulate_grad\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[0;32m    274\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 14
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
