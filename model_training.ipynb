{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-08-22T16:06:24.739040Z",
     "start_time": "2024-08-22T16:06:24.600733Z"
    }
   },
   "source": [
    "import cv2\n",
    "import torch\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\" # I use AMD... ðŸ˜­\n",
    "device"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cpu'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Initialize dataset directory",
   "id": "cb24d90f21c08e10"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-22T16:06:27.162457Z",
     "start_time": "2024-08-22T16:06:27.157529Z"
    }
   },
   "cell_type": "code",
   "source": [
    "train_dir = ''\n",
    "test_dir = ''   \n",
    "val_dir = ''"
   ],
   "id": "5e01ec44c313d9c8",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Initialize dataset",
   "id": "3908f1236557f8f8"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "One of the data necessary is the content of the image. To achieve this, we will have to generate an image with plain white background, using Verily Serif Mono font.",
   "id": "b08d4f51bbdada08"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-22T16:06:29.043501Z",
     "start_time": "2024-08-22T16:06:28.901471Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from PIL import Image, ImageDraw, ImageFont\n",
    "def draw_word(word: str) -> Image:\n",
    "    text_len = len(word)\n",
    "    font_size = 50\n",
    "    w = max(256, int(text_len * font_size * 0.64))\n",
    "    h = 256\n",
    "    \n",
    "    img = Image.new('RGB', (w,h), color=(255, 255, 255))\n",
    "    font = ImageFont.truetype('VerilySerifMono.otf', font_size)\n",
    "    d = ImageDraw.Draw(img)\n",
    "    text_width, text_height = d.textsize(word, font)\n",
    "    position = ((w - text_width) / 2, (h-text_height) / 2)\n",
    "    \n",
    "    d.text(position, word, font=font, fill=0)\n",
    "    return img"
   ],
   "id": "e0f89d2c3e51d128",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-22T16:06:33.066971Z",
     "start_time": "2024-08-22T16:06:30.942808Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import json\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import random\n",
    "\n",
    "class ImageDataset(Dataset):\n",
    "    def __init__(self, image_dir, word_dir):\n",
    "        self.image_dir = image_dir\n",
    "        with open(word_dir, 'r') as f:\n",
    "            self.words = json.load(f)\n",
    "        self.images = list(self.image_dir.glob('*.jpg'))\n",
    "        self.transform = transforms.Compose([\n",
    "            transforms.Lambda(lambda img: cv2.cvtColor(img, cv2.COLOR_BGR2RGB)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Resize((256, 256))\n",
    "        ])\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        assert idx <= len(self), 'Index out of range'\n",
    "        try:\n",
    "            rgb_img = self.transform(Image.open(self.images[idx]).convert('RGB'))\n",
    "            \n",
    "            content = random.choice(list(self.words.values()))\n",
    "            allowed_symbols = '0123456789abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ!\\\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'\n",
    "            content = ''.join([i for i in content if i in allowed_symbols])\n",
    "            while not content:\n",
    "                content = random.choice(list(self.words.values()))\n",
    "                content = ''.join([i for i in content if i in allowed_symbols])\n",
    "            img_content = self.transform(draw_word(content))\n",
    "            \n",
    "            content_style = self.words[self.images[idx].stem]\n",
    "            content_style = ''.join([i for i in content_style if i in allowed_symbols])\n",
    "            if not content_style:\n",
    "                content_style = 'o'\n",
    "            img_content_style = self.transform(draw_word(content_style))\n",
    "            return rgb_img, img_content, content, img_content_style, content_style\n",
    "        except Exception as e:\n",
    "            return torch.tensor(-1), torch.tensor(-1)\n",
    "    \n",
    "        # item = {'image': img, 'idx': idx, 'label': self.words[self.images[idx].name]}\n",
    "        # return item"
   ],
   "id": "362acd3572fd2161",
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Loss function",
   "id": "582dd2ca84b52d59"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 1. VGG19 Typeface classifier, C - Text Perceptual Loss\n",
    "- Perceptual loss computed from the feature maps at layer i denoted as Ï†i and Mi is the number of elements in the particular feature map which is used as normalization. Only computed for output image corresponding to original content.\n",
    "- Texture loss / style loss computed from Gram matrix of the feature maps.\n",
    "- Embedding-based loss computed from feature maps of the penultimate layer of this network. (???)\n",
    "\n",
    "https://gist.github.com/alper111/8233cdb0414b4cb5853f2f730ab95a49#gistcomment-3347450"
   ],
   "id": "9ee389dd203780c2"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-15T02:36:08.578782Z",
     "start_time": "2024-08-15T02:36:08.569600Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Paper trains this model with Synth-Font dataset, however I could not find it\n",
    "class VGGPerceptualLoss(torch.nn.Module):\n",
    "    def __init__(self, resize=True):\n",
    "        super(VGGPerceptualLoss, self).__init__()\n",
    "        blocks= [torchvision.models.vgg19(pretrained=True).features[:4].eval(),\n",
    "                 torchvision.models.vgg19(pretrained=True).features[4:9].eval(),\n",
    "                 torchvision.models.vgg19(pretrained=True).features[9:16].eval(),\n",
    "                 torchvision.models.vgg19(pretrained=True).features[16:23].eval()]\n",
    "        for bl in blocks:\n",
    "            for p in bl.parameters():   \n",
    "                p.requires_grad = False\n",
    "        self.blocks = torch.nn.ModuleList(blocks)\n",
    "        self.transform = torch.nn.functional.interpolate\n",
    "        self.resize = resize\n",
    "        self.register_buffer('mean', torch.tensor([0.485, 0.456, 0.406]).view(1, 3, 1, 1))\n",
    "        self.register_buffer('std', torch.tensor([0.229, 0.224, 0.225]).view(1, 3, 1, 1))\n",
    "    \n",
    "    # default: feature_layers=[0, 1, 2, 3], style_layers=[]    \n",
    "    def forward(self, prediction, target, feature_layers=None, style_layers=None):    \n",
    "        if style_layers is None:\n",
    "            style_layers = [0, 1, 2, 3]\n",
    "        if feature_layers is None:\n",
    "            feature_layers = [2]\n",
    "        if prediction.shape[1] != 3:\n",
    "            prediction = prediction.repeat(1, 3, 1, 1)\n",
    "            target = target.repeat(1, 3, 1, 1)\n",
    "        prediction = (prediction - self.mean) / self.std\n",
    "        target = (target-self.mean)/self.std\n",
    "        if self.resize:\n",
    "            prediction = self.transform(prediction, mode='bilinear', size=(224, 224), align_corners=False)\n",
    "            target = self.transform(target, mode='bilinear', size=(224, 224), align_corners=False)\n",
    "        loss = 0.0\n",
    "        x = prediction\n",
    "        y = target\n",
    "        for i, block in enumerate(self.blocks):\n",
    "            x = block(x)\n",
    "            y = block(y)\n",
    "            if i in feature_layers:\n",
    "                loss += torch.nn.functional.l1_loss(x, y)\n",
    "            if i in style_layers:\n",
    "                act_x = x.reshape(x.shape[0], x.shape[1], -1)\n",
    "                act_y = y.reshape(y.shape[0], y.shape[1], -1)\n",
    "                gram_x = act_x @ act_x.permute(0, 2, 1)\n",
    "                gram_y = act_y @ act_y.permute(0, 2, 1)\n",
    "                loss += torch.nn.functional.l1_loss(gram_x, gram_y)\n",
    "        return loss"
   ],
   "id": "283cb2611445278",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 2. OCR, R - Text Content Loss\n",
    "- The relevant modules has already been added along with the saved configuration mentioned in the paper, and the PyTorch model can be downloaded [here](https://drive.google.com/file/d/1b59rXuGGmKne1AuHnkgDzoYgKeETNMv9/view?usp=drive_link). Have not implemented it to calculate loss yet.\n",
    "- The content loss is computed by measuring the cross entropy between the sequence of characters in the input string, c1, c2, the predicted string, c'1, c'2 respectively and are represented as one-hot vectors.\n",
    "\n",
    "https://github.com/clovaai/deep-text-recognition-benchmark"
   ],
   "id": "dd0b230c5ac13059"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-22T16:06:41.281141Z",
     "start_time": "2024-08-22T16:06:41.208238Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from torch import nn\n",
    "from ocr.utils import AttnLabelConverter\n",
    "from ocr.model import Model\n",
    "\n",
    "class opt:\n",
    "    # Hardcoding the arguments instead of using argument parsers \n",
    "    # as per https://github.com/clovaai/deep-text-recognition-benchmark/blob/master/demo.py#L96\n",
    "    image_folder = 'images'\n",
    "    workers = 4\n",
    "    batch_size = 256\n",
    "    saved_model = 'ocr/TPS-ResNet-BiLSTM-Attn.pth'\n",
    "    batch_max_length = 25\n",
    "    imgH = 32\n",
    "    imgW = 100\n",
    "    rgb = False # See input_channel comment below\n",
    "    character = '0123456789abcdefghijklmnopqrstuvwxyz'\n",
    "    sensitive = True\n",
    "    PAD = True\n",
    "    Transformation = 'TPS'\n",
    "    FeatureExtraction = 'ResNet'\n",
    "    SequenceModeling = 'BiLSTM'\n",
    "    Prediction = 'Attn'\n",
    "    num_fiducial = 20\n",
    "    input_channel = 1\n",
    "    output_channel = 512\n",
    "    hidden_size = 256\n",
    "    \n",
    "class OCRLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(OCRLoss, self).__init__()\n",
    "        \n",
    "        self.converter = AttnLabelConverter(opt.character)\n",
    "        self.opt = opt()\n",
    "        self.opt.num_class = len(self.converter.character)\n",
    "        \n",
    "        if self.opt.rgb:\n",
    "            self.opt.input_channel = 3 # This breaks loading, input_channel has to be 1, or rgb false\n",
    "        \n",
    "        self.model = Model(opt)\n",
    "        self.model = torch.nn.DataParallel(self.model).to(device)\n",
    "        \n",
    "        mappings = torch.load(opt.saved_model, map_location=device)\n",
    "        self.model.load_state_dict(mappings)\n",
    "        self.model.eval()\n",
    "        self.criterion = nn.CrossEntropyLoss(ignore_index=True) # Unsure what setting for this\n",
    "        \n",
    "        \n",
    "    def forward(self, image, label):\n",
    "        batch_size = image.size(0)\n",
    "        text = torch.LongTensor(batch_size, opt.batch_max_length +1).fill_(0).to(device)\n",
    "        preds = self.model(image, text, isTrain=False)\n",
    "        loss = self.criterion(\n",
    "            preds.view(-1, preds.shape[-1]), \n",
    "            label.contiguous().view(-1)\n",
    "        )\n",
    "        \n",
    "        return loss"
   ],
   "id": "a8bcda5e268eb161",
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# 1. Style Encoder\n",
    "This is a homebrew version of ResNet34 architecture,I have no idea if this works, but I am prepared to alter it in the events it fails. "
   ],
   "id": "d1f723138fb31fd5"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-14T16:19:38.498336Z",
     "start_time": "2024-08-14T16:19:38.493168Z"
    }
   },
   "cell_type": "code",
   "outputs": [],
   "execution_count": 18,
   "source": [
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3)\n",
    "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3)\n",
    "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
    "        \n",
    "        self.shortcut = nn.Sequential()\n",
    "        if in_channels != out_channels:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_channels, out_channels, kernel_size=1),\n",
    "                nn.BatchNorm2d(out_channels)\n",
    "            )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.bn2(self.conv2(out))\n",
    "        out += self.shortcut(x)\n",
    "        out = self.relu(out)\n",
    "        return out"
   ],
   "id": "a24a8d6eb648d084"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from torch import nn\n",
    "import torchvision\n",
    "class StyleEncoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(StyleEncoder, self).__init__()\n",
    "        self.layer_stack = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=3, out_channels=32, kernel_size=3), #256x256\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2), #128x128\n",
    "            self._make_layer(self, ResidualBlock, 64, 128, stride=2),\n",
    "            nn.Conv2d(in_channels=128, out_channels=128, kernel_size=3),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2), #64x64\n",
    "            self._make_layer(self, ResidualBlock, 128, 256, stride=2),\n",
    "            nn.Conv2d(in_channels=256, out_channels=256, kernel_size=3),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2), #32x32\n",
    "            self._make_layer(self, ResidualBlock, 256, 512, stride=2),\n",
    "            nn.Conv2d(in_channels=512, out_channels=512, kernel_size=3),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2), #16x16\n",
    "            self._make_layer(self, ResidualBlock, 512, 512, stride=2),\n",
    "            nn.Conv2d(in_channels=512, out_channels=512, kernel_size=3, stride=1),\n",
    "        )\n",
    "        \n",
    "    def _make_layer(self, block, in_channels, out_channels, num_blocks, stride):\n",
    "        strides = [stride] + [1]*(num_blocks-1)\n",
    "        layers = []\n",
    "        for stride in strides:\n",
    "            layers.append(block(in_channels, out_channels, stride))\n",
    "            self.in_channels = out_channels\n",
    "        return nn.Sequential(*layers)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return torchvision.ops.roi_align(input=self.layer_stack(x)) #1x1"
   ],
   "id": "8494437a481da690",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 2. Content Encoder",
   "id": "4f8f2ccfcc8fe60"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from torch import nn\n",
    "import torchvision\n",
    "class ContentEncoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ContentEncoder, self).__init__()\n",
    "        self.layer_stack = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=3, out_channels=32, kernel_size=3), #256x256\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2), #128x128\n",
    "            self._make_layer(self, ResidualBlock, 64, 128, stride=2),\n",
    "            nn.Conv2d(in_channels=128, out_channels=128, kernel_size=3),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2), #64x64\n",
    "            self._make_layer(self, ResidualBlock, 128, 256, stride=2),\n",
    "            nn.Conv2d(in_channels=256, out_channels=256, kernel_size=3),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2), #32x32\n",
    "            self._make_layer(self, ResidualBlock, 256, 512, stride=2),\n",
    "            nn.Conv2d(in_channels=512, out_channels=512, kernel_size=3),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2), #16x16\n",
    "            self._make_layer(self, ResidualBlock, 512, 512, stride=2),\n",
    "            nn.Conv2d(in_channels=512, out_channels=512, kernel_size=3, stride=1),\n",
    "        )\n",
    "        \n",
    "    def _make_layer(self, block, in_channels, out_channels, num_blocks, stride):\n",
    "        strides = [stride] + [1]*(num_blocks-1)\n",
    "        layers = []\n",
    "        for stride in strides:\n",
    "            layers.append(block(in_channels, out_channels, stride))\n",
    "            self.in_channels = out_channels\n",
    "        return nn.Sequential(*layers)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.layer_stack(x)"
   ],
   "id": "cd1be39e7aa8c18d"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# 3. Style Mapping Network\n",
    "- Converts style representations to layer-specific style representations which are then fed as AdaIN normalization coefficient to each layer of the generator. \n",
    "-  eliminate the use of the noise vector input of the standard StyleGAN2"
   ],
   "id": "e5de306ee9b9e446"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-22T16:06:56.330614Z",
     "start_time": "2024-08-22T16:06:56.322691Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import functools\n",
    "class NLayerDiscriminator(nn.Module):\n",
    "    \"\"\"Defines a PatchGAN discriminator\"\"\"\n",
    "\n",
    "    def __init__(self, input_nc, ndf=64, n_layers=3, norm_layer=nn.BatchNorm2d):\n",
    "        \"\"\"Construct a PatchGAN discriminator\n",
    "        Parameters:\n",
    "            input_nc (int)  -- the number of channels in input images\n",
    "            ndf (int)       -- the number of filters in the last conv layer\n",
    "            n_layers (int)  -- the number of conv layers in the discriminator\n",
    "            norm_layer      -- normalization layer\n",
    "        \"\"\"\n",
    "        super(NLayerDiscriminator, self).__init__()\n",
    "        if type(norm_layer) == functools.partial:  # no need to use bias as BatchNorm2d has affine parameters\n",
    "            use_bias = norm_layer.func == nn.InstanceNorm2d\n",
    "        else:\n",
    "            use_bias = norm_layer == nn.InstanceNorm2d\n",
    "\n",
    "        kw = 4\n",
    "        padw = 1\n",
    "        sequence = [nn.Conv2d(input_nc, ndf, kernel_size=kw, stride=2, padding=padw), nn.LeakyReLU(0.2, True)]\n",
    "        nf_mult = 1\n",
    "        nf_mult_prev = 1\n",
    "        for n in range(1, n_layers):  # gradually increase the number of filters\n",
    "            nf_mult_prev = nf_mult\n",
    "            nf_mult = min(2 ** n, 8)\n",
    "            sequence += [\n",
    "                nn.Conv2d(ndf * nf_mult_prev, ndf * nf_mult, kernel_size=kw, stride=2, padding=padw, bias=use_bias),\n",
    "                norm_layer(ndf * nf_mult),\n",
    "                nn.LeakyReLU(0.2, True)\n",
    "            ]\n",
    "\n",
    "        nf_mult_prev = nf_mult\n",
    "        nf_mult = min(2 ** n_layers, 8)\n",
    "        sequence += [\n",
    "            nn.Conv2d(ndf * nf_mult_prev, ndf * nf_mult, kernel_size=kw, stride=1, padding=padw, bias=use_bias),\n",
    "            norm_layer(ndf * nf_mult),\n",
    "            nn.LeakyReLU(0.2, True)\n",
    "        ]\n",
    "\n",
    "        sequence += [nn.Conv2d(ndf * nf_mult, 1, kernel_size=kw, stride=1, padding=padw)]  # output 1 channel prediction map\n",
    "        self.model = nn.Sequential(*sequence)\n",
    "\n",
    "    def forward(self, input):\n",
    "        \"\"\"Standard forward.\"\"\"\n",
    "        return self.model(input)"
   ],
   "id": "a3e59f3c3be6e829",
   "outputs": [],
   "execution_count": 12
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Training",
   "id": "3693c2743a2205a8"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-22T16:07:03.898959Z",
     "start_time": "2024-08-22T16:07:03.883356Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class StyleGanTrainer:\n",
    "    def __init__(self,\n",
    "                 model_G: nn.Module,\n",
    "                 model_D: nn.Module,\n",
    "                 style_encoder: nn.Module,\n",
    "                 content_encoder: nn.Module,\n",
    "                 optimizer_G: torch.optim.Optimizer,\n",
    "                 optimizer_D: torch.optim.Optimizer,\n",
    "                 scheduler_G: torch.optim.lr_scheduler.LRScheduler,\n",
    "                 scheduler_D: torch.optim.lr_scheduler.LRScheduler,\n",
    "                 train_dataloader: torch.utils.data.DataLoader,\n",
    "                 val_dataloader: torch.utils.data.DataLoader,\n",
    "                 total_epochs: int,\n",
    "                 ocr_loss: nn.Module,\n",
    "                 # typeface_loss: nn.Module,\n",
    "                 perc_loss: nn.Module,\n",
    "                 cons_loss: nn.Module,\n",
    "                 adv_loss: nn.Module,\n",
    "                 device: str):\n",
    "        \n",
    "        self.style_encoder = style_encoder,\n",
    "        self.content_encoder = content_encoder,\n",
    "        self.model_G = model_G,\n",
    "        self.model_D = model_D,\n",
    "        self.optimizer_G = optimizer_G,\n",
    "        self.optimizer_D = optimizer_D,\n",
    "        self.scheduler_G = scheduler_G,\n",
    "        self.scheduler_D = scheduler_D,\n",
    "        self.train_dataloader = train_dataloader,\n",
    "        self.val_dataloader = val_dataloader,\n",
    "        self.device = device\n",
    "        self.total_epochs = total_epochs,\n",
    "        # self.typeface_loss = typeface_loss.to(device)\n",
    "        self.ocr_loss = ocr_loss.to(device)\n",
    "        self.perc_loss = perc_loss.to(device)\n",
    "        self.cons_loss = cons_loss.to(device)\n",
    "        self.adv_loss = adv_loss.to(device)\n",
    "        \n",
    "    def set_requires_grad(self, net: nn.Module, requires_grad: bool=False):\n",
    "        if net is not None:\n",
    "            for param in net.parameters():\n",
    "                param.requires_grad = requires_grad\n",
    "                \n",
    "    def model_D_loss(self, style_img: torch.Tensor, content_encodes: torch.Tensor, style_encodes: torch.Tensor):\n",
    "        pred_D_fake = self.model_D(self.model_G(content_encodes, style_encodes).detach())\n",
    "        pred_D_real = self.model_D(style_img)\n",
    "        fake = torch.tensor(0.).expand_as(pred_D_fake).to(self.device)\n",
    "        real = torch.tensor(1.).expand_as(pred_D_real).to(self.device)\n",
    "        return (self.adv_loss(pred_D_real, real) + self.adv_loss(pred_D_fake, fake)) / 2.\n",
    "    \n",
    "    def model_G_loss(self, preds: torch.Tensor):\n",
    "        pred_D_fake = self.model_D(preds)\n",
    "        valid = torch.tensor(1.).expand_as(pred_D_fake).to(self.device)\n",
    "        return self.adv_loss(pred_D_fake, valid)\n",
    "                \n",
    "    def train(self):\n",
    "        print(\"Start training...\")\n",
    "        self.model_G.train()\n",
    "        self.model_D.train()\n",
    "        self.style_encoder.train()\n",
    "        self.content_encoder.train()\n",
    "        \n",
    "        for style_img, desired_content, desired_labels, style_content, style_labels in self.train_dataloader:\n",
    "            if max(len(label) for label in desired_labels) > 25:\n",
    "                continue\n",
    "            if max(len(label) for label in style_labels) > 25:\n",
    "                continue\n",
    "            \n",
    "            self.optimizer_G.zero_grad()\n",
    "            self.optimizer_D.zero_grad()\n",
    "            \n",
    "            style_img = style_img.to(self.device)\n",
    "            desired_content = desired_content.to(self.device)\n",
    "            style_content = style_content.to(self.device)\n",
    "            \n",
    "            style_encodes = self.style_encoder(style_img)\n",
    "            content_encodes = self.content_encoder(desired_content)\n",
    "            style_content_encodes = self.content_encoder(style_content)\n",
    "            \n",
    "            ## Calculate D loss\n",
    "            self.set_requires_grad(self.model_D, True)\n",
    "            self.set_requires_grad(self.model_G, False)\n",
    "            loss_D = self.model_D_loss(style_img, style_content_encodes, style_encodes)\n",
    "            \n",
    "            ## Calculate G loss\n",
    "            self.set_requires_grad(self.model_D, False)\n",
    "            self.set_requires_grad(self.model_G, True)\n",
    "            \n",
    "            preds = self.model_G(content_encodes, style_encodes)\n",
    "            ocr_loss = self.ocr_loss(preds, desired_labels)\n",
    "            \n",
    "            reconstructed = self.model_G(style_content_encodes, style_encodes)\n",
    "            reconstructed_loss = self.cons_loss(style_img, reconstructed)\n",
    "            \n",
    "            reconstructed_style_encode = self.style_encoder(reconstructed)\n",
    "            cycled = model_G(style_content_encodes, reconstructed_style_encode)\n",
    "            cycled_loss = self.cons_loss(style_img, cycled)\n",
    "            \n",
    "            ocr_loss_rec = self.ocr_loss(reconstructed, style_labels)\n",
    "            ocr_loss_total = (ocr_loss + ocr_loss_rec) / 2.\n",
    "            \n",
    "            perc_loss, tex_loss = self.perc_loss(style_img, reconstructed)\n",
    "            enc_loss = 0  # self.typeface_loss(style_img, reconstructed)\n",
    "            \n",
    "            adv_loss = self.model_G_loss(reconstructed)\n",
    "            \n",
    "            loss_G = 0.07 * ocr_loss_total + 2.0 * cycled_loss + 2.0 * reconstructed_loss + 25.0 * perc_loss + 7.0 * tex_loss + 0 * enc_loss + 0.06 * adv_loss\n",
    "            \n",
    "            self.set_requires_grad(self.model_D, True)\n",
    "            \n",
    "            loss_G.backward()\n",
    "            loss_D.backward()\n",
    "            \n",
    "            self.optimizer_D.step()\n",
    "            self.optimizer_G.step()\n",
    "            \n",
    "    def validate(self, epoch: int):\n",
    "        print(\"Start validating...\")\n",
    "        self.model_G.eval()\n",
    "        self.model_D.eval()\n",
    "        self.style_encoder.eval()\n",
    "        self.content_encoder.eval()\n",
    "        \n",
    "        for style_img, desired_content, desired_labels, style_content, style_labels in self.val_dataloader:\n",
    "            if max(len(label) for label in desired_labels) > 25:\n",
    "                continue\n",
    "            if max(len(label) for label in style_labels) > 25:\n",
    "                continue\n",
    "            \n",
    "            self.optimizer_G.zero_grad()\n",
    "            self.optimizer_D.zero_grad()\n",
    "            \n",
    "            style_img = style_img.to(self.device)\n",
    "            desired_content = desired_content.to(self.device)\n",
    "            style_content = style_content.to(self.device)\n",
    "            style_encodes = self.style_encoder(style_img)\n",
    "            content_encodes = self.content_encoder(desired_content)\n",
    "            \n",
    "            preds = self.model_G(content_encodes, style_encodes)\n",
    "            ocr_loss = self.ocr_loss(preds, desired_labels)\n",
    "            \n",
    "            style_label_encodes = self.content_encoder(style_labels)\n",
    "            \n",
    "            reconstructed = model_G(style_label_encodes, style_encodes)\n",
    "            reconstructed_loss = self.cons_loss(style_img, reconstructed)\n",
    "            \n",
    "            reconstructed_style_encode = self.style_encoder(reconstructed)\n",
    "            cycle = model_G(style_label_encodes, reconstructed_style_encode)\n",
    "            cycle_loss = self.cons_loss(style_img, cycle)\n",
    "            \n",
    "            ocr_loss_rec = self.ocr_loss(reconstructed, style_labels)\n",
    "            ocr_loss_total = (ocr_loss + ocr_loss_rec) / 2.\n",
    "            \n",
    "            perc_loss, tex_loss = self.perc_loss(style_img, preds)\n",
    "            enc_loss = 0 # self.typeface_loss(style_img, preds)\n",
    "            adv_loss = self.model_G_loss(reconstructed)\n",
    "            \n",
    "            loss = 0.07 * ocr_loss_total + 2.0 * cycle_loss + 2.0 * reconstructed_loss + 25.0 * perc_loss + 7.0 * tex_loss + 0 * enc_loss + 0.06 * adv_loss\n",
    "                \n",
    "    def run(self):\n",
    "        for epoch in range(self.total_epochs):\n",
    "            self.train()\n",
    "            with torch.no_grad():\n",
    "                self.validate(epoch)\n",
    "            if self.scheduler_G is not None:\n",
    "                self.scheduler_G.step()\n",
    "            if self.scheduler_D is not None:\n",
    "                self.scheduler_D.step()"
   ],
   "id": "ff8d4f70838fd74b",
   "outputs": [],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-22T16:06:17.949449Z",
     "start_time": "2024-08-22T16:06:17.888637Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from stylegan import StyleBased_Generator\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "total_epochs = 500\n",
    "\n",
    "batch_size = 16\n",
    "train_dataloader = DataLoader(ImageDataset(train_dir, train_dir+'/words.json'), shuffle=True, batch_size=batch_size)\n",
    "val_dataloader = DataLoader(ImageDataset(train_dir, train_dir+'/words.json'), batch_size=batch_size)\n",
    "\n",
    "model_G = StyleBased_Generator(dim_latent=512)\n",
    "model_G.to(device)\n",
    "style_encoder = StyleEncoder().to(device)\n",
    "content_encoder = ContentEncoder().to(device)\n",
    "model_D = NLayerDiscriminator(input_nc=3, ndf=64, n_layers=3, norm_layer=(lambda x : torch.nn.Identity()))\n",
    "model_D.to(device)\n",
    "\n",
    "optimizer_G = torch.optim.AdamW(\n",
    "    list(model_G.parameters()) + \n",
    "    list(style_encoder.parameters()) +\n",
    "    list(style_encoder.parameters()),\n",
    "    lr=1e-3,\n",
    "    weight_decay=1e-6\n",
    ")\n",
    "scheduler_G = torch.optim.lr_scheduler.ExponentialLR(\n",
    "    optimizer_G,\n",
    "    gamma=0.9\n",
    ")\n",
    "optimizer_D = torch.optim.AdamW(model_D.parameters(), lr=1e-4)\n",
    "scheduler_D = torch.optim.lr_scheduler.ExponentialLR(\n",
    "    optimizer_D,\n",
    "    gamma=0.9\n",
    ")\n",
    "\n",
    "trainer = StyleGanTrainer(  \n",
    "    model_G,\n",
    "    model_D,\n",
    "    style_encoder,\n",
    "    content_encoder,\n",
    "    optimizer_G,\n",
    "    optimizer_D,\n",
    "    scheduler_G,\n",
    "    scheduler_D,\n",
    "    train_dataloader,\n",
    "    val_dataloader,\n",
    "    total_epochs=total_epochs,\n",
    "    ocr_loss=OCRLoss(),\n",
    "    perc_loss=VGGPerceptualLoss(),\n",
    "    cons_loss=torch.nn.L1Loss(),\n",
    "    adv_loss=torch.nn.MSELoss(),\n",
    "    device=device,\n",
    ")\n",
    "\n",
    "trainer.run()"
   ],
   "id": "67426edacbea3aac",
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'torch' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[6], line 3\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mstylegan\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m StyleBased_Generator\n\u001B[1;32m----> 3\u001B[0m device \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcuda\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[43mtorch\u001B[49m\u001B[38;5;241m.\u001B[39mcuda\u001B[38;5;241m.\u001B[39mis_available() \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcpu\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m      4\u001B[0m total_epochs \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m500\u001B[39m\n\u001B[0;32m      6\u001B[0m batch_size \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m16\u001B[39m\n",
      "\u001B[1;31mNameError\u001B[0m: name 'torch' is not defined"
     ]
    }
   ],
   "execution_count": 6
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
