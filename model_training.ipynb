{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-08-14T19:14:57.388756Z",
     "start_time": "2024-08-14T19:14:57.375626Z"
    }
   },
   "source": [
    "import cv2\n",
    "import torch\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\" # I use AMD... ðŸ˜­\n",
    "device"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cpu'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Initialize dataset directory",
   "id": "cb24d90f21c08e10"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-14T16:10:46.750959Z",
     "start_time": "2024-08-14T16:10:46.747783Z"
    }
   },
   "cell_type": "code",
   "source": [
    "train_dir = ''\n",
    "test_dir = ''\n",
    "val_dir = ''"
   ],
   "id": "5e01ec44c313d9c8",
   "outputs": [],
   "execution_count": 12
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Initializing dataset",
   "id": "3908f1236557f8f8"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-14T16:18:55.713384Z",
     "start_time": "2024-08-14T16:18:55.502221Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import json\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "\n",
    "class ImageDataset(Dataset):\n",
    "    def __init__(self, image_dir, word_dir):\n",
    "        self.image_dir = image_dir\n",
    "        with open(word_dir, 'r') as f:\n",
    "            self.words = json.load(f)\n",
    "        self.images = list(self.image_dir.glob('*.jpg'))\n",
    "        self.transform = transforms.Compose([\n",
    "            transforms.Lambda(lambda img: cv2.cvtColor(img, cv2.COLOR_BGR2RGB)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Resize((256, 256))\n",
    "        ])\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        assert idx <= len(self), 'Index out of range'\n",
    "        try:\n",
    "            rgb_img = self.transform(Image.open(self.images[idx]).convert('RGB'))\n",
    "            bw_img = self.transform(Image.open(self.images[idx]).convert('L'))\n",
    "            return rgb_img, bw_img\n",
    "        except Exception as e:\n",
    "            return torch.tensor(-1), torch.tensor(-1)\n",
    "    \n",
    "        # item = {'image': img, 'idx': idx, 'label': self.words[self.images[idx].name]}\n",
    "        # return item"
   ],
   "id": "362acd3572fd2161",
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'torch' has no attribute 'version'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mAttributeError\u001B[0m                            Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[17], line 3\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mjson\u001B[39;00m\n\u001B[0;32m      2\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtorch\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mutils\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mdata\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m Dataset\n\u001B[1;32m----> 3\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtorchvision\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m transforms\n\u001B[0;32m      4\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mPIL\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m Image\n\u001B[0;32m      6\u001B[0m \u001B[38;5;28;01mclass\u001B[39;00m \u001B[38;5;21;01mImageDataset\u001B[39;00m(Dataset):\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torchvision\\__init__.py:10\u001B[0m\n\u001B[0;32m      7\u001B[0m \u001B[38;5;66;03m# Don't re-order these, we need to load the _C extension (done when importing\u001B[39;00m\n\u001B[0;32m      8\u001B[0m \u001B[38;5;66;03m# .extensions) before entering _meta_registrations.\u001B[39;00m\n\u001B[0;32m      9\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mextension\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m _HAS_OPS  \u001B[38;5;66;03m# usort:skip\u001B[39;00m\n\u001B[1;32m---> 10\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtorchvision\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m _meta_registrations, datasets, io, models, ops, transforms, utils  \u001B[38;5;66;03m# usort:skip\u001B[39;00m\n\u001B[0;32m     12\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m     13\u001B[0m     \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mversion\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m __version__  \u001B[38;5;66;03m# noqa: F401\u001B[39;00m\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torchvision\\models\\__init__.py:2\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01malexnet\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;241m*\u001B[39m\n\u001B[1;32m----> 2\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mconvnext\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;241m*\u001B[39m\n\u001B[0;32m      3\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mdensenet\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;241m*\u001B[39m\n\u001B[0;32m      4\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mefficientnet\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;241m*\u001B[39m\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torchvision\\models\\convnext.py:9\u001B[0m\n\u001B[0;32m      6\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtorch\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mnn\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m functional \u001B[38;5;28;01mas\u001B[39;00m F\n\u001B[0;32m      8\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mops\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mmisc\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m Conv2dNormActivation, Permute\n\u001B[1;32m----> 9\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mops\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mstochastic_depth\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m StochasticDepth\n\u001B[0;32m     10\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mtransforms\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m_presets\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m ImageClassification\n\u001B[0;32m     11\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mutils\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m _log_api_usage_once\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torchvision\\ops\\__init__.py:23\u001B[0m\n\u001B[0;32m     21\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mgiou_loss\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m generalized_box_iou_loss\n\u001B[0;32m     22\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mmisc\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m Conv2dNormActivation, Conv3dNormActivation, FrozenBatchNorm2d, MLP, Permute, SqueezeExcitation\n\u001B[1;32m---> 23\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpoolers\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m MultiScaleRoIAlign\n\u001B[0;32m     24\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mps_roi_align\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m ps_roi_align, PSRoIAlign\n\u001B[0;32m     25\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mps_roi_pool\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m ps_roi_pool, PSRoIPool\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torchvision\\ops\\poolers.py:10\u001B[0m\n\u001B[0;32m      7\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtorchvision\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mops\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mboxes\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m box_area\n\u001B[0;32m      9\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mutils\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m _log_api_usage_once\n\u001B[1;32m---> 10\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mroi_align\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m roi_align\n\u001B[0;32m     13\u001B[0m \u001B[38;5;66;03m# copying result_idx_in_level to a specific index in result[]\u001B[39;00m\n\u001B[0;32m     14\u001B[0m \u001B[38;5;66;03m# is not supported by ONNX tracing yet.\u001B[39;00m\n\u001B[0;32m     15\u001B[0m \u001B[38;5;66;03m# _onnx_merge_levels() is an implementation supported by ONNX\u001B[39;00m\n\u001B[0;32m     16\u001B[0m \u001B[38;5;66;03m# that merges the levels to the right indices\u001B[39;00m\n\u001B[0;32m     17\u001B[0m \u001B[38;5;129m@torch\u001B[39m\u001B[38;5;241m.\u001B[39mjit\u001B[38;5;241m.\u001B[39munused\n\u001B[0;32m     18\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_onnx_merge_levels\u001B[39m(levels: Tensor, unmerged_results: List[Tensor]) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Tensor:\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torchvision\\ops\\roi_align.py:7\u001B[0m\n\u001B[0;32m      5\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mtorch\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mfx\u001B[39;00m\n\u001B[0;32m      6\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtorch\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m nn, Tensor\n\u001B[1;32m----> 7\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtorch\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m_dynamo\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mutils\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m is_compile_supported\n\u001B[0;32m      8\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtorch\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mjit\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mannotations\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m BroadcastingList2\n\u001B[0;32m      9\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtorch\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mnn\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mmodules\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mutils\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m _pair\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\_dynamo\\__init__.py:2\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mtorch\u001B[39;00m\n\u001B[1;32m----> 2\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m convert_frame, eval_frame, resume_execution\n\u001B[0;32m      3\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mbackends\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mregistry\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m list_backends, lookup_backend, register_backend\n\u001B[0;32m      4\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mcallback\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m callback_handler, on_compile_end, on_compile_start\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:48\u001B[0m\n\u001B[0;32m     45\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtorch\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mutils\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m_python_dispatch\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m _disable_current_modes\n\u001B[0;32m     46\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtorch\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mutils\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m_traceback\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m format_traceback_short\n\u001B[1;32m---> 48\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m config, exc, trace_rules\n\u001B[0;32m     49\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mbackends\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mregistry\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m CompilerFn\n\u001B[0;32m     50\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mbytecode_analysis\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m remove_dead_code, remove_pointless_jumps\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\_dynamo\\config.py:101\u001B[0m\n\u001B[0;32m     98\u001B[0m allow_ignore_mark_dynamic \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mFalse\u001B[39;00m\n\u001B[0;32m    100\u001B[0m \u001B[38;5;66;03m# Set this to False to assume nn.Modules() contents are immutable (similar assumption as freezing)\u001B[39;00m\n\u001B[1;32m--> 101\u001B[0m guard_nn_modules \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mFalse\u001B[39;00m \u001B[38;5;28;01mif\u001B[39;00m \u001B[43mis_fbcode\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[0;32m    103\u001B[0m \u001B[38;5;66;03m# Uses CPython internal dictionary tags to detect mutation. There is some\u001B[39;00m\n\u001B[0;32m    104\u001B[0m \u001B[38;5;66;03m# overlap between guard_nn_modules_using_dict_tags and guard_nn_modules flag.\u001B[39;00m\n\u001B[0;32m    105\u001B[0m \u001B[38;5;66;03m# guard_nn_modules unspecializes the nn module instance and adds guard for each\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    113\u001B[0m \u001B[38;5;66;03m# TODO(janimesh, voz): Remove both of these flags (or atleast guard_nn_modules)\u001B[39;00m\n\u001B[0;32m    114\u001B[0m \u001B[38;5;66;03m# once we have reached stability for the guard_nn_modules_using_dict_tags.\u001B[39;00m\n\u001B[0;32m    115\u001B[0m guard_nn_modules_using_dict_tags \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\_dynamo\\config.py:15\u001B[0m, in \u001B[0;36mis_fbcode\u001B[1;34m()\u001B[0m\n\u001B[0;32m     14\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mis_fbcode\u001B[39m():\n\u001B[1;32m---> 15\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28mhasattr\u001B[39m(\u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mversion\u001B[49m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mgit_version\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\__init__.py:2216\u001B[0m, in \u001B[0;36m__getattr__\u001B[1;34m(name)\u001B[0m\n\u001B[0;32m   2213\u001B[0m     \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mimportlib\u001B[39;00m\n\u001B[0;32m   2214\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m importlib\u001B[38;5;241m.\u001B[39mimport_module(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m.\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mname\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;18m__name__\u001B[39m)\n\u001B[1;32m-> 2216\u001B[0m \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mAttributeError\u001B[39;00m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmodule \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;18m__name__\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m has no attribute \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mname\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "\u001B[1;31mAttributeError\u001B[0m: module 'torch' has no attribute 'version'"
     ]
    }
   ],
   "execution_count": 17
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Loss function",
   "id": "582dd2ca84b52d59"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 1. VGG19 Typface classifier, C - Text Perceptual Loss\n",
    "- Perceptual loss computed from the feature maps at layer i denoted as Ï†i and Mi is the number of elements in the particular feature map which is used as normalization. Only computed for output image corresponding to original content.\n",
    "- Texture loss / style loss computed from Gram matrix of the feature maps.\n",
    "- Embedding-based loss computed from feature maps of the penultimate layer of this network. (???)\n",
    "\n",
    "https://gist.github.com/alper111/8233cdb0414b4cb5853f2f730ab95a49#gistcomment-3347450"
   ],
   "id": "9ee389dd203780c2"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-14T16:17:12.721917Z",
     "start_time": "2024-08-14T16:17:12.713158Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Paper trains this model with Synth-Font dataset, however I could not find it\n",
    "class VGGPerceptualLoss(torch.nn.Module):\n",
    "    def __init__(self, resize=True):\n",
    "        super(VGGPerceptualLoss, self).__init__()\n",
    "        blocks= [torchvision.models.vgg19(pretrained=True).features[:4].eval(),\n",
    "                 torchvision.models.vgg19(pretrained=True).features[4:9].eval(),\n",
    "                 torchvision.models.vgg19(pretrained=True).features[9:16].eval(),\n",
    "                 torchvision.models.vgg19(pretrained=True).features[16:23].eval()]\n",
    "        for bl in blocks:\n",
    "            for p in bl.parameters():   \n",
    "                p.requires_grad = False\n",
    "        self.blocks = torch.nn.ModuleList(blocks)\n",
    "        self.transform = torch.nn.functional.interpolate\n",
    "        self.resize = resize\n",
    "        self.register_buffer('mean', torch.tensor([0.485, 0.456, 0.406]).view(1, 3, 1, 1))\n",
    "        self.register_buffer('std', torch.tensor([0.229, 0.224, 0.225]).view(1, 3, 1, 1))\n",
    "    \n",
    "    # default: feature_layers=[0, 1, 2, 3], style_layers=[]    \n",
    "    def forward(self, prediction, target, feature_layers=None, style_layers=None):    \n",
    "        if style_layers is None:\n",
    "            style_layers = [0, 1, 2, 3]\n",
    "        if feature_layers is None:\n",
    "            feature_layers = [2]\n",
    "        if prediction.shape[1] != 3:\n",
    "            prediction = prediction.repeat(1, 3, 1, 1)\n",
    "            target = target.repeat(1, 3, 1, 1)\n",
    "        prediction = (prediction - self.mean) / self.std\n",
    "        target = (target-self.mean)/self.std\n",
    "        if self.resize:\n",
    "            prediction = self.transform(prediction, mode='bilinear', size=(224, 224), align_corners=False)\n",
    "            target = self.transform(target, mode='bilinear', size=(224, 224), align_corners=False)\n",
    "        loss = 0.0\n",
    "        x = prediction\n",
    "        y = target\n",
    "        for i, block in enumerate(self.blocks):\n",
    "            x = block(x)\n",
    "            y = block(y)\n",
    "            if i in feature_layers:\n",
    "                loss += torch.nn.functional.l1_loss(x, y)\n",
    "            if i in style_layers:\n",
    "                act_x = x.reshape(x.shape[0], x.shape[1], -1)\n",
    "                act_y = y.reshape(y.shape[0], y.shape[1], -1)\n",
    "                gram_x = act_x @ act_x.permute(0, 2, 1)\n",
    "                gram_y = act_y @ act_y.permute(0, 2, 1)\n",
    "                loss += torch.nn.functional.l1_loss(gram_x, gram_y)\n",
    "        return loss"
   ],
   "id": "283cb2611445278",
   "outputs": [],
   "execution_count": 15
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 2. OCR, R - Text Content Loss\n",
    "- The relevant modules and model file has already been added, along with the saved configuration mentioned in the paper. Have not implemented it to calculate loss yet.\n",
    "- The content loss is computed by measuring the cross entropy between the sequence of characters in the input string, c1, c2, the predicted string, c'1, c'2 respectively and are represented as one-hot vectors.\n",
    "\n",
    "https://github.com/clovaai/deep-text-recognition-benchmark"
   ],
   "id": "dd0b230c5ac13059"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-14T16:00:50.721060Z",
     "start_time": "2024-08-14T16:00:50.221501Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import ocr\n",
    "import argparse\n",
    "from ocr.utils import AttnLabelConverter\n",
    "from ocr.model import Model\n",
    "\n",
    "class opt():\n",
    "    saved_model = 'ocr/TPS-ResNet-BiLSTM-Attn.pth'\n",
    "    character = '0123456789abcdefghijklmnopqrstuvwxyz'\n",
    "    Transformation = 'TPS'\n",
    "    FeatureExtraction = 'ResNet'\n",
    "    SequenceModeling = 'BiLSTM'\n",
    "    Prediction = 'Attn'\n",
    "    rgb = True\n",
    "    \n",
    "converter = AttnLabelConverter(opt.character)\n",
    "opt.num_class = len(converter.character)\n",
    "\n",
    "if opt.rgb:\n",
    "    opt.input_channel = 3\n",
    "model = Model(opt)\n",
    "model = torch.nn.DataParallel(model).to(device)\n",
    "\n",
    "model.load_state_dict(torch.load(opt.saved_model, map_location=device))\n",
    "# Need to load data, presumably through DataLoader(). Idk hows the input data like yet, so i hold this.\n",
    "#demo_loader = torch.utils.data.DataLoader(\n",
    "#       demo_data, batch_size=opt.batch_size,\n",
    "#       shuffle=False,\n",
    "#       num_workers=int(opt.workers),\n",
    "#       collate_fn=AlignCollate_demo, pin_memory=True)\n",
    "model.eval()\n",
    "# It's getting late, idk anymore, over to you\n"
   ],
   "id": "a8bcda5e268eb161",
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[1], line 3\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mocr\u001B[39;00m\n\u001B[0;32m      2\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01margparse\u001B[39;00m\n\u001B[1;32m----> 3\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mocr\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mutils\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m AttnLabelConverter\n\u001B[0;32m      4\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mocr\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mmodel\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m Model\n\u001B[0;32m      6\u001B[0m \u001B[38;5;28;01mclass\u001B[39;00m \u001B[38;5;21;01mopt\u001B[39;00m():\n",
      "File \u001B[1;32m~\\PycharmProjects\\IMGUR5K-Handwriting-Dataset\\ocr\\utils.py:1\u001B[0m\n\u001B[1;32m----> 1\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mtorch\u001B[39;00m\n\u001B[0;32m      2\u001B[0m device \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mdevice(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mcuda\u001B[39m\u001B[38;5;124m'\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m torch\u001B[38;5;241m.\u001B[39mcuda\u001B[38;5;241m.\u001B[39mis_available() \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mcpu\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[0;32m      5\u001B[0m \u001B[38;5;28;01mclass\u001B[39;00m \u001B[38;5;21;01mCTCLabelConverter\u001B[39;00m(\u001B[38;5;28mobject\u001B[39m):\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\__init__.py:132\u001B[0m\n\u001B[0;32m    130\u001B[0m is_loaded \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mFalse\u001B[39;00m\n\u001B[0;32m    131\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m with_load_library_flags:\n\u001B[1;32m--> 132\u001B[0m     res \u001B[38;5;241m=\u001B[39m \u001B[43mkernel32\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mLoadLibraryExW\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdll\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m0x00001100\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[0;32m    133\u001B[0m     last_error \u001B[38;5;241m=\u001B[39m ctypes\u001B[38;5;241m.\u001B[39mget_last_error()\n\u001B[0;32m    134\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m res \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m last_error \u001B[38;5;241m!=\u001B[39m \u001B[38;5;241m126\u001B[39m:\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from torch import nn\n",
    "class CombinedLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CombinedLoss, self).__init__()\n",
    "    \n",
    "    def forward(self, predictions, target):\n",
    "        "
   ],
   "id": "945cefc6b55fd68d"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# 1. Style Encoder\n",
    "This is a homebrew version of ResNet34 architecture,I have no idea if this works, but I am prepared to alter it in the events it fails. "
   ],
   "id": "d1f723138fb31fd5"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-14T16:19:38.498336Z",
     "start_time": "2024-08-14T16:19:38.493168Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3)\n",
    "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3)\n",
    "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
    "        \n",
    "        self.shortcut = nn.Sequential()\n",
    "        if in_channels != out_channels:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_channels, out_channels, kernel_size=1),\n",
    "                nn.BatchNorm2d(out_channels)\n",
    "            )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.bn2(self.conv2(out))\n",
    "        out += self.shortcut(x)\n",
    "        out = self.relu(out)\n",
    "        return out"
   ],
   "id": "a24a8d6eb648d084",
   "outputs": [],
   "execution_count": 18
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from torch import nn\n",
    "import torchvision\n",
    "class StyleEncoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(StyleEncoder, self).__init__()\n",
    "        self.layer_stack = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=3, out_channels=32, kernel_size=3), #256x256\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2), #128x128\n",
    "            self._make_layer(self, ResidualBlock, 64, 128, stride=2),\n",
    "            nn.Conv2d(in_channels=128, out_channels=128, kernel_size=3),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2), #64x64\n",
    "            self._make_layer(self, ResidualBlock, 128, 256, stride=2),\n",
    "            nn.Conv2d(in_channels=256, out_channels=256, kernel_size=3),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2), #32x32\n",
    "            self._make_layer(self, ResidualBlock, 256, 512, stride=2),\n",
    "            nn.Conv2d(in_channels=512, out_channels=512, kernel_size=3),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2), #16x16\n",
    "            self._make_layer(self, ResidualBlock, 512, 512, stride=2),\n",
    "            nn.Conv2d(in_channels=512, out_channels=512, kernel_size=3, stride=1),\n",
    "        )\n",
    "        \n",
    "    def _make_layer(self, block, in_channels, out_channels, num_blocks, stride):\n",
    "        strides = [stride] + [1]*(num_blocks-1)\n",
    "        layers = []\n",
    "        for stride in strides:\n",
    "            layers.append(block(in_channels, out_channels, stride))\n",
    "            self.in_channels = out_channels\n",
    "        return nn.Sequential(*layers)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return torchvision.ops.roi_align(input=self.layer_stack(x)) #1x1"
   ],
   "id": "8494437a481da690",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
