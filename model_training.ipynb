{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-08-25T16:43:36.358870Z",
     "start_time": "2024-08-25T16:43:36.354841Z"
    }
   },
   "source": [
    "import cv2\n",
    "import torch\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\" # I use AMD... ðŸ˜­\n",
    "device"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cpu'"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 62
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Initialize dataset directory",
   "id": "cb24d90f21c08e10"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-25T16:43:36.404649Z",
     "start_time": "2024-08-25T16:43:36.401869Z"
    }
   },
   "cell_type": "code",
   "source": [
    "train_dir = ''\n",
    "test_dir = '' \n",
    "val_dir = ''"
   ],
   "id": "5e01ec44c313d9c8",
   "outputs": [],
   "execution_count": 63
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Initialize dataset",
   "id": "3908f1236557f8f8"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "One of the data necessary is the content of the image. To achieve this, we will have to generate an image with plain white background, using Verily Serif Mono font.",
   "id": "b08d4f51bbdada08"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-25T16:43:36.409944Z",
     "start_time": "2024-08-25T16:43:36.405655Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from PIL import Image, ImageDraw, ImageFont\n",
    "def draw_word(word: str) -> Image:\n",
    "    text_len = len(word)\n",
    "    font_size = 50\n",
    "    w = max(256, int(text_len * font_size * 0.64))\n",
    "    h = 256\n",
    "    \n",
    "    img = Image.new('RGB', (w,h), color=(255, 255, 255))\n",
    "    font = ImageFont.truetype('VerilySerifMono.otf', font_size)\n",
    "    d = ImageDraw.Draw(img)\n",
    "    text_width, text_height = d.textsize(word, font)\n",
    "    position = ((w - text_width) / 2, (h-text_height) / 2)\n",
    "    \n",
    "    d.text(position, word, font=font, fill=0)\n",
    "    return img"
   ],
   "id": "e0f89d2c3e51d128",
   "outputs": [],
   "execution_count": 64
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-25T16:43:36.419215Z",
     "start_time": "2024-08-25T16:43:36.410949Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import random\n",
    "\n",
    "class ImageDataset(Dataset):\n",
    "    def __init__(self, image_dir, word_dir):\n",
    "        self.image_dir = Path(image_dir)\n",
    "        with open(word_dir, 'r') as f:\n",
    "            self.words = json.load(f)\n",
    "        self.images = list(self.image_dir.glob('*.jpg'))\n",
    "        self.transform = transforms.Compose([\n",
    "            transforms.Lambda(lambda img: cv2.cvtColor(img, cv2.COLOR_BGR2RGB)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Resize((256, 256))\n",
    "        ])\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        assert idx <= len(self), 'Index out of range'\n",
    "        try:\n",
    "            rgb_img = self.transform(Image.open(self.images[idx]).convert('RGB'))\n",
    "            \n",
    "            content = random.choice(list(self.words.values()))\n",
    "            allowed_symbols = '0123456789abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ!\\\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'\n",
    "            content = ''.join([i for i in content if i in allowed_symbols])\n",
    "            while not content:\n",
    "                content = random.choice(list(self.words.values()))\n",
    "                content = ''.join([i for i in content if i in allowed_symbols])\n",
    "            img_content = self.transform(draw_word(content))\n",
    "            \n",
    "            content_style = self.words[self.images[idx].stem]\n",
    "            content_style = ''.join([i for i in content_style if i in allowed_symbols])\n",
    "            if not content_style:\n",
    "                content_style = 'o'\n",
    "            img_content_style = self.transform(draw_word(content_style))\n",
    "            return rgb_img, img_content, content, img_content_style, content_style\n",
    "        except Exception as e:\n",
    "            return torch.tensor(-1), torch.tensor(-1)\n",
    "    \n",
    "        # item = {'image': img, 'idx': idx, 'label': self.words[self.images[idx].name]}\n",
    "        # return item"
   ],
   "id": "362acd3572fd2161",
   "outputs": [],
   "execution_count": 65
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Loss function",
   "id": "582dd2ca84b52d59"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 1. VGG19 Typeface classifier, C - Text Perceptual Loss\n",
    "- Perceptual loss computed from the feature maps at layer i denoted as Ï†i and Mi is the number of elements in the particular feature map which is used as normalization. Only computed for output image corresponding to original content.\n",
    "- Texture loss / style loss computed from Gram matrix of the feature maps.\n",
    "- Embedding-based loss computed from feature maps of the penultimate layer of this network. (???)\n",
    "\n",
    "https://gist.github.com/alper111/8233cdb0414b4cb5853f2f730ab95a49#gistcomment-3347450"
   ],
   "id": "9ee389dd203780c2"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-25T16:43:36.428306Z",
     "start_time": "2024-08-25T16:43:36.420215Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Paper trains this model with Synth-Font dataset, however I could not find it\n",
    "class VGGPerceptualLoss(torch.nn.Module):\n",
    "    def __init__(self, resize=True):\n",
    "        super(VGGPerceptualLoss, self).__init__()\n",
    "        blocks= [torchvision.models.vgg19(pretrained=True).features[:4].eval(),\n",
    "                 torchvision.models.vgg19(pretrained=True).features[4:9].eval(),\n",
    "                 torchvision.models.vgg19(pretrained=True).features[9:16].eval(),\n",
    "                 torchvision.models.vgg19(pretrained=True).features[16:23].eval()]\n",
    "        for bl in blocks:\n",
    "            for p in bl.parameters():   \n",
    "                p.requires_grad = False\n",
    "        self.blocks = torch.nn.ModuleList(blocks)\n",
    "        self.transform = torch.nn.functional.interpolate\n",
    "        self.resize = resize\n",
    "        self.register_buffer('mean', torch.tensor([0.485, 0.456, 0.406]).view(1, 3, 1, 1))\n",
    "        self.register_buffer('std', torch.tensor([0.229, 0.224, 0.225]).view(1, 3, 1, 1))\n",
    "    \n",
    "    # default: feature_layers=[0, 1, 2, 3], style_layers=[]    \n",
    "    def forward(self, prediction, target, feature_layers=None, style_layers=None):    \n",
    "        if style_layers is None:\n",
    "            style_layers = [0, 1, 2, 3]\n",
    "        if feature_layers is None:\n",
    "            feature_layers = [2]\n",
    "        if prediction.shape[1] != 3:\n",
    "            prediction = prediction.repeat(1, 3, 1, 1)\n",
    "            target = target.repeat(1, 3, 1, 1)\n",
    "        prediction = (prediction - self.mean) / self.std\n",
    "        target = (target-self.mean)/self.std\n",
    "        if self.resize:\n",
    "            prediction = self.transform(prediction, mode='bilinear', size=(224, 224), align_corners=False)\n",
    "            target = self.transform(target, mode='bilinear', size=(224, 224), align_corners=False)\n",
    "        loss = 0.0\n",
    "        x = prediction\n",
    "        y = target\n",
    "        for i, block in enumerate(self.blocks):\n",
    "            x = block(x)\n",
    "            y = block(y)\n",
    "            if i in feature_layers:\n",
    "                loss += torch.nn.functional.l1_loss(x, y)\n",
    "            if i in style_layers:\n",
    "                act_x = x.reshape(x.shape[0], x.shape[1], -1)\n",
    "                act_y = y.reshape(y.shape[0], y.shape[1], -1)\n",
    "                gram_x = act_x @ act_x.permute(0, 2, 1)\n",
    "                gram_y = act_y @ act_y.permute(0, 2, 1)\n",
    "                loss += torch.nn.functional.l1_loss(gram_x, gram_y)\n",
    "        return loss"
   ],
   "id": "283cb2611445278",
   "outputs": [],
   "execution_count": 66
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 2. OCR, R - Text Content Loss\n",
    "- The relevant modules has already been added along with the saved configuration mentioned in the paper, and the PyTorch model can be downloaded [here](https://drive.google.com/file/d/1b59rXuGGmKne1AuHnkgDzoYgKeETNMv9/view?usp=drive_link). Have not implemented it to calculate loss yet.\n",
    "- The content loss is computed by measuring the cross entropy between the sequence of characters in the input string, c1, c2, the predicted string, c'1, c'2 respectively and are represented as one-hot vectors.\n",
    "\n",
    "https://github.com/clovaai/deep-text-recognition-benchmark"
   ],
   "id": "dd0b230c5ac13059"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-25T16:43:36.436253Z",
     "start_time": "2024-08-25T16:43:36.429474Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from torch import nn\n",
    "from ocr.utils import AttnLabelConverter\n",
    "from ocr.model import Model\n",
    "\n",
    "class opt:\n",
    "    # Hardcoding the arguments instead of using argument parsers \n",
    "    # as per https://github.com/clovaai/deep-text-recognition-benchmark/blob/master/demo.py#L96\n",
    "    image_folder = 'images'\n",
    "    workers = 4\n",
    "    batch_size = 256\n",
    "    saved_model = 'ocr/TPS-ResNet-BiLSTM-Attn.pth'\n",
    "    batch_max_length = 25\n",
    "    imgH = 32\n",
    "    imgW = 100\n",
    "    rgb = False # See input_channel comment below\n",
    "    character = '0123456789abcdefghijklmnopqrstuvwxyz'\n",
    "    sensitive = True\n",
    "    PAD = True\n",
    "    Transformation = 'TPS'\n",
    "    FeatureExtraction = 'ResNet'\n",
    "    SequenceModeling = 'BiLSTM'\n",
    "    Prediction = 'Attn'\n",
    "    num_fiducial = 20\n",
    "    input_channel = 1\n",
    "    output_channel = 512\n",
    "    hidden_size = 256\n",
    "    num_class = 0\n",
    "    \n",
    "class OCRLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(OCRLoss, self).__init__()\n",
    "        \n",
    "        self.converter = AttnLabelConverter(opt.character)\n",
    "        self.opt = opt()\n",
    "        self.opt.num_class = len(self.converter.character)\n",
    "        \n",
    "        if self.opt.rgb:\n",
    "            self.opt.input_channel = 3 # This breaks loading, input_channel has to be 1, or rgb false\n",
    "        \n",
    "        self.model = Model(opt)\n",
    "        self.model = torch.nn.DataParallel(self.model).to(device)\n",
    "        \n",
    "        mappings = torch.load(opt.saved_model, map_location=device)\n",
    "        self.model.load_state_dict(mappings)\n",
    "        self.model.eval()\n",
    "        self.criterion = nn.CrossEntropyLoss(ignore_index=True) # Unsure what setting for this\n",
    "        \n",
    "        \n",
    "    def forward(self, image, label):\n",
    "        batch_size = image.size(0)\n",
    "        text = torch.LongTensor(batch_size, opt.batch_max_length +1).fill_(0).to(device)\n",
    "        preds = self.model(image, text, isTrain=False)\n",
    "        loss = self.criterion(\n",
    "            preds.view(-1, preds.shape[-1]), \n",
    "            label.contiguous().view(-1)\n",
    "        )\n",
    "        \n",
    "        return loss"
   ],
   "id": "a8bcda5e268eb161",
   "outputs": [],
   "execution_count": 67
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# 1. Style Encoder\n",
    "This is a homebrew version of ResNet34 architecture,I have no idea if this works, but I am prepared to alter it in the events it fails. "
   ],
   "id": "d1f723138fb31fd5"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-25T16:43:36.440293Z",
     "start_time": "2024-08-25T16:43:36.437258Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# class ResidualBlock(nn.Module):\n",
    "#     def __init__(self, in_channels, out_channels):\n",
    "#         super(ResidualBlock, self).__init__()\n",
    "#         self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3)\n",
    "#         self.bn1 = nn.BatchNorm2d(out_channels)\n",
    "#         self.relu = nn.ReLU()\n",
    "#         self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3)\n",
    "#         self.bn2 = nn.BatchNorm2d(out_channels)\n",
    "#         \n",
    "#         self.shortcut = nn.Sequential()\n",
    "#         if in_channels != out_channels:\n",
    "#             self.shortcut = nn.Sequential(\n",
    "#                 nn.Conv2d(in_channels, out_channels, kernel_size=1),\n",
    "#                 nn.BatchNorm2d(out_channels)\n",
    "#             )\n",
    "#         \n",
    "#     def forward(self, x):\n",
    "#         out = self.relu(self.bn1(self.conv1(x)))\n",
    "#         out = self.bn2(self.conv2(out))\n",
    "#         out += self.shortcut(x)\n",
    "#         out = self.relu(out)\n",
    "#         return out"
   ],
   "id": "a24a8d6eb648d084",
   "outputs": [],
   "execution_count": 68
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-25T16:43:36.444334Z",
     "start_time": "2024-08-25T16:43:36.441292Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# from torch import nn\n",
    "# import torchvision\n",
    "# class StyleEncoder(nn.Module):\n",
    "#     def __init__(self):\n",
    "#         super(StyleEncoder, self).__init__()\n",
    "#         self.layer_stack = nn.Sequential(\n",
    "#             nn.Conv2d(in_channels=3, out_channels=32, kernel_size=3), #256x256\n",
    "#             nn.ReLU(inplace=True),\n",
    "#             nn.BatchNorm2d(32),\n",
    "#             nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3),\n",
    "#             nn.ReLU(inplace=True),\n",
    "#             nn.BatchNorm2d(64),\n",
    "#             nn.MaxPool2d(kernel_size=2, stride=2), #128x128\n",
    "#             self._make_layer(ResidualBlock, 64, 128, stride=2),\n",
    "#             nn.Conv2d(in_channels=128, out_channels=128, kernel_size=3),\n",
    "#             nn.ReLU(inplace=True),\n",
    "#             nn.BatchNorm2d(128),\n",
    "#             nn.MaxPool2d(kernel_size=2, stride=2), #64x64\n",
    "#             self._make_layer(ResidualBlock, 128, 256, stride=2),\n",
    "#             nn.Conv2d(in_channels=256, out_channels=256, kernel_size=3),\n",
    "#             nn.ReLU(inplace=True),\n",
    "#             nn.BatchNorm2d(256),\n",
    "#             nn.MaxPool2d(kernel_size=2, stride=2), #32x32\n",
    "#             self._make_layer(ResidualBlock, 256, 512, stride=2),\n",
    "#             nn.Conv2d(in_channels=512, out_channels=512, kernel_size=3),\n",
    "#             nn.ReLU(inplace=True),\n",
    "#             nn.BatchNorm2d(512),\n",
    "#             nn.MaxPool2d(kernel_size=2, stride=2), #16x16\n",
    "#             self._make_layer(ResidualBlock, 512, 512, stride=2),\n",
    "#             nn.Conv2d(in_channels=512, out_channels=512, kernel_size=3, stride=1),\n",
    "#         )\n",
    "#         \n",
    "#     def _make_layer(self, block: ResidualBlock, in_channels, out_channels, num_blocks, stride):\n",
    "#         strides = [stride] + [1]*(num_blocks-1)\n",
    "#         layers = []\n",
    "#         for stride in strides:\n",
    "#             layers.append(block(in_channels, out_channels))\n",
    "#             self.in_channels = out_channels\n",
    "#         return nn.Sequential(*layers)\n",
    "#     \n",
    "#     def forward(self, x):\n",
    "#         return torchvision.ops.roi_align(input=self.layer_stack(x)) #1x1"
   ],
   "id": "8494437a481da690",
   "outputs": [],
   "execution_count": 69
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 2. Content Encoder",
   "id": "4f8f2ccfcc8fe60"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-25T16:43:36.469965Z",
     "start_time": "2024-08-25T16:43:36.466432Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# from torch import nn\n",
    "# import torchvision\n",
    "# class ContentEncoder(nn.Module):\n",
    "#     def __init__(self):\n",
    "#         super(ContentEncoder, self).__init__()\n",
    "#         self.layer_stack = nn.Sequential(\n",
    "#             nn.Conv2d(in_channels=3, out_channels=32, kernel_size=3), #256x256\n",
    "#             nn.ReLU(inplace=True),\n",
    "#             nn.BatchNorm2d(32),\n",
    "#             nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3),\n",
    "#             nn.ReLU(inplace=True),\n",
    "#             nn.BatchNorm2d(64),\n",
    "#             nn.MaxPool2d(kernel_size=2, stride=2), #128x128\n",
    "#             self._make_layer(self, ResidualBlock, 64, 128, stride=2),\n",
    "#             nn.Conv2d(in_channels=128, out_channels=128, kernel_size=3),\n",
    "#             nn.ReLU(inplace=True),\n",
    "#             nn.BatchNorm2d(128),\n",
    "#             nn.MaxPool2d(kernel_size=2, stride=2), #64x64\n",
    "#             self._make_layer(self, ResidualBlock, 128, 256, stride=2),\n",
    "#             nn.Conv2d(in_channels=256, out_channels=256, kernel_size=3),\n",
    "#             nn.ReLU(inplace=True),\n",
    "#             nn.BatchNorm2d(256),\n",
    "#             nn.MaxPool2d(kernel_size=2, stride=2), #32x32\n",
    "#             self._make_layer(self, ResidualBlock, 256, 512, stride=2),\n",
    "#             nn.Conv2d(in_channels=512, out_channels=512, kernel_size=3),\n",
    "#             nn.ReLU(inplace=True),\n",
    "#             nn.BatchNorm2d(512),\n",
    "#             nn.MaxPool2d(kernel_size=2, stride=2), #16x16\n",
    "#             self._make_layer(self, ResidualBlock, 512, 512, stride=2),\n",
    "#             nn.Conv2d(in_channels=512, out_channels=512, kernel_size=3, stride=1),\n",
    "#         )\n",
    "#         \n",
    "#     def _make_layer(self, block, in_channels, out_channels, num_blocks, stride):\n",
    "#         strides = [stride] + [1]*(num_blocks-1)\n",
    "#         layers = []\n",
    "#         for stride in strides:\n",
    "#             layers.append(block(in_channels, out_channels, stride))\n",
    "#             self.in_channels = out_channels\n",
    "#         return nn.Sequential(*layers)\n",
    "#     \n",
    "#     def forward(self, x):\n",
    "#         return self.layer_stack(x)"
   ],
   "id": "cd1be39e7aa8c18d",
   "outputs": [],
   "execution_count": 70
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-25T16:43:36.486785Z",
     "start_time": "2024-08-25T16:43:36.481363Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from torchvision.ops import roi_align\n",
    "from torchvision import models\n",
    "from torchvision.models.resnet import BasicBlock\n",
    "\n",
    "class ContentEncoder(models.ResNet):\n",
    "    def __init__(self):\n",
    "        # resnet18 init\n",
    "        super().__init__(BasicBlock, [2, 2, 2, 2])\n",
    "\n",
    "    def _forward_impl(self, x):\n",
    "        # See note [TorchScript super()]\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.maxpool(x)\n",
    "\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "\n",
    "        #x = self.avgpool(x)\n",
    "        #x = torch.flatten(x, 1)\n",
    "        #x = self.fc(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class StyleEncoder(models.ResNet):\n",
    "    def __init__(self):\n",
    "        # resnet18 init\n",
    "        super().__init__(BasicBlock, [2, 2, 2, 2])\n",
    "        self.fc = torch.nn.Identity()"
   ],
   "id": "49c9449a4fdc6861",
   "outputs": [],
   "execution_count": 71
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# 3. Style Mapping Network\n",
    "- Converts style representations to layer-specific style representations which are then fed as AdaIN normalization coefficient to each layer of the generator. \n",
    "-  eliminate the use of the noise vector input of the standard StyleGAN2"
   ],
   "id": "e5de306ee9b9e446"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-25T16:43:36.494892Z",
     "start_time": "2024-08-25T16:43:36.487791Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import functools\n",
    "class NLayerDiscriminator(nn.Module):\n",
    "    \"\"\"Defines a PatchGAN discriminator\"\"\"\n",
    "\n",
    "    def __init__(self, input_nc, ndf=64, n_layers=3, norm_layer=nn.BatchNorm2d):\n",
    "        \"\"\"Construct a PatchGAN discriminator\n",
    "        Parameters:\n",
    "            input_nc (int)  -- the number of channels in input images\n",
    "            ndf (int)       -- the number of filters in the last conv layer\n",
    "            n_layers (int)  -- the number of conv layers in the discriminator\n",
    "            norm_layer      -- normalization layer\n",
    "        \"\"\"\n",
    "        super(NLayerDiscriminator, self).__init__()\n",
    "        if type(norm_layer) == functools.partial:  # no need to use bias as BatchNorm2d has affine parameters\n",
    "            use_bias = norm_layer.func == nn.InstanceNorm2d\n",
    "        else:\n",
    "            use_bias = norm_layer == nn.InstanceNorm2d\n",
    "\n",
    "        kw = 4\n",
    "        padw = 1\n",
    "        sequence = [nn.Conv2d(input_nc, ndf, kernel_size=kw, stride=2, padding=padw), nn.LeakyReLU(0.2, True)]\n",
    "        nf_mult = 1\n",
    "        nf_mult_prev = 1\n",
    "        for n in range(1, n_layers):  # gradually increase the number of filters\n",
    "            nf_mult_prev = nf_mult\n",
    "            nf_mult = min(2 ** n, 8)\n",
    "            sequence += [\n",
    "                nn.Conv2d(ndf * nf_mult_prev, ndf * nf_mult, kernel_size=kw, stride=2, padding=padw, bias=use_bias),\n",
    "                norm_layer(ndf * nf_mult),\n",
    "                nn.LeakyReLU(0.2, True)\n",
    "            ]\n",
    "\n",
    "        nf_mult_prev = nf_mult\n",
    "        nf_mult = min(2 ** n_layers, 8)\n",
    "        sequence += [\n",
    "            nn.Conv2d(ndf * nf_mult_prev, ndf * nf_mult, kernel_size=kw, stride=1, padding=padw, bias=use_bias),\n",
    "            norm_layer(ndf * nf_mult),\n",
    "            nn.LeakyReLU(0.2, True)\n",
    "        ]\n",
    "\n",
    "        sequence += [nn.Conv2d(ndf * nf_mult, 1, kernel_size=kw, stride=1, padding=padw)]  # output 1 channel prediction map\n",
    "        self.model = nn.Sequential(*sequence)\n",
    "\n",
    "    def forward(self, input):\n",
    "        \"\"\"Standard forward.\"\"\"\n",
    "        return self.model(input)"
   ],
   "id": "a3e59f3c3be6e829",
   "outputs": [],
   "execution_count": 72
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Training",
   "id": "3693c2743a2205a8"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-25T16:43:36.511335Z",
     "start_time": "2024-08-25T16:43:36.496237Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class StyleGanTrainer:\n",
    "    def __init__(self,\n",
    "                 model_G: nn.Module,\n",
    "                 model_D: nn.Module,\n",
    "                 style_encoder: nn.Module,\n",
    "                 content_encoder: nn.Module,\n",
    "                 optimizer_G: torch.optim.Optimizer,\n",
    "                 optimizer_D: torch.optim.Optimizer,\n",
    "                 scheduler_G: torch.optim.lr_scheduler.LRScheduler,\n",
    "                 scheduler_D: torch.optim.lr_scheduler.LRScheduler,\n",
    "                 train_dataloader: torch.utils.data.DataLoader,\n",
    "                 val_dataloader: torch.utils.data.DataLoader,\n",
    "                 total_epochs: int,\n",
    "                 ocr_loss: nn.Module,\n",
    "                 # typeface_loss: nn.Module,\n",
    "                 perc_loss: nn.Module,\n",
    "                 cons_loss: nn.Module,\n",
    "                 adv_loss: nn.Module,\n",
    "                 device: str):\n",
    "        \n",
    "        self.style_encoder = style_encoder,\n",
    "        self.content_encoder = content_encoder,\n",
    "        self.model_G = model_G,\n",
    "        self.model_D = model_D,\n",
    "        self.optimizer_G = optimizer_G,\n",
    "        self.optimizer_D = optimizer_D,\n",
    "        self.scheduler_G = scheduler_G,\n",
    "        self.scheduler_D = scheduler_D,\n",
    "        self.train_dataloader = train_dataloader,\n",
    "        self.val_dataloader = val_dataloader,\n",
    "        self.device = device\n",
    "        self.total_epochs = total_epochs,\n",
    "        # self.typeface_loss = typeface_loss.to(device)\n",
    "        self.ocr_loss = ocr_loss.to(device)\n",
    "        self.perc_loss = perc_loss.to(device)\n",
    "        self.cons_loss = cons_loss.to(device)\n",
    "        self.adv_loss = adv_loss.to(device)\n",
    "        \n",
    "    def set_requires_grad(self, net: nn.Module, requires_grad: bool=False):\n",
    "        if net is not None:\n",
    "            for param in net.parameters():\n",
    "                param.requires_grad = requires_grad\n",
    "                \n",
    "    def model_D_loss(self, style_img: torch.Tensor, content_encodes: torch.Tensor, style_encodes: torch.Tensor):\n",
    "        pred_D_fake = self.model_D(self.model_G(content_encodes, style_encodes).detach())\n",
    "        pred_D_real = self.model_D(style_img)\n",
    "        fake = torch.tensor(0.).expand_as(pred_D_fake).to(self.device)\n",
    "        real = torch.tensor(1.).expand_as(pred_D_real).to(self.device)\n",
    "        return (self.adv_loss(pred_D_real, real) + self.adv_loss(pred_D_fake, fake)) / 2.\n",
    "    \n",
    "    def model_G_loss(self, preds: torch.Tensor):\n",
    "        pred_D_fake = self.model_D(preds)\n",
    "        valid = torch.tensor(1.).expand_as(pred_D_fake).to(self.device)\n",
    "        return self.adv_loss(pred_D_fake, valid)\n",
    "                \n",
    "    def train(self):\n",
    "        print(\"Start training...\")\n",
    "        self.model_G.train()\n",
    "        self.model_D.train()\n",
    "        self.style_encoder.train()\n",
    "        self.content_encoder.train()\n",
    "        \n",
    "        for style_img, desired_content, desired_labels, style_content, style_labels in self.train_dataloader:\n",
    "            if max(len(label) for label in desired_labels) > 25:\n",
    "                continue\n",
    "            if max(len(label) for label in style_labels) > 25:\n",
    "                continue\n",
    "            \n",
    "            self.optimizer_G.zero_grad()\n",
    "            self.optimizer_D.zero_grad()\n",
    "            \n",
    "            style_img = style_img.to(self.device)\n",
    "            desired_content = desired_content.to(self.device)\n",
    "            style_content = style_content.to(self.device)\n",
    "            \n",
    "            style_encodes = self.style_encoder(style_img)\n",
    "            content_encodes = self.content_encoder(desired_content)\n",
    "            style_content_encodes = self.content_encoder(style_content)\n",
    "            \n",
    "            ## Calculate D loss\n",
    "            self.set_requires_grad(self.model_D, True)\n",
    "            self.set_requires_grad(self.model_G, False)\n",
    "            loss_D = self.model_D_loss(style_img, style_content_encodes, style_encodes)\n",
    "            \n",
    "            ## Calculate G loss\n",
    "            self.set_requires_grad(self.model_D, False)\n",
    "            self.set_requires_grad(self.model_G, True)\n",
    "            \n",
    "            preds = self.model_G(content_encodes, style_encodes)\n",
    "            ocr_loss = self.ocr_loss(preds, desired_labels)\n",
    "            \n",
    "            reconstructed = self.model_G(style_content_encodes, style_encodes)\n",
    "            reconstructed_loss = self.cons_loss(style_img, reconstructed)\n",
    "            \n",
    "            reconstructed_style_encode = self.style_encoder(reconstructed)\n",
    "            cycled = model_G(style_content_encodes, reconstructed_style_encode)\n",
    "            cycled_loss = self.cons_loss(style_img, cycled)\n",
    "            \n",
    "            ocr_loss_rec = self.ocr_loss(reconstructed, style_labels)\n",
    "            ocr_loss_total = (ocr_loss + ocr_loss_rec) / 2.\n",
    "            \n",
    "            perc_loss, tex_loss = self.perc_loss(style_img, reconstructed)\n",
    "            enc_loss = 0  # self.typeface_loss(style_img, reconstructed)\n",
    "            \n",
    "            adv_loss = self.model_G_loss(reconstructed)\n",
    "            \n",
    "            loss_G = 0.07 * ocr_loss_total + 2.0 * cycled_loss + 2.0 * reconstructed_loss + 25.0 * perc_loss + 7.0 * tex_loss + 0 * enc_loss + 0.06 * adv_loss\n",
    "            \n",
    "            self.set_requires_grad(self.model_D, True)\n",
    "            \n",
    "            loss_G.backward()\n",
    "            loss_D.backward()\n",
    "            \n",
    "            self.optimizer_D.step()\n",
    "            self.optimizer_G.step()\n",
    "            \n",
    "    def validate(self, epoch: int):\n",
    "        print(\"Start validating...\")\n",
    "        self.model_G.eval()\n",
    "        self.model_D.eval()\n",
    "        self.style_encoder.eval()\n",
    "        self.content_encoder.eval()\n",
    "        \n",
    "        for style_img, desired_content, desired_labels, style_content, style_labels in self.val_dataloader:\n",
    "            if max(len(label) for label in desired_labels) > 25:\n",
    "                continue\n",
    "            if max(len(label) for label in style_labels) > 25:\n",
    "                continue\n",
    "            \n",
    "            self.optimizer_G.zero_grad()\n",
    "            self.optimizer_D.zero_grad()\n",
    "            \n",
    "            style_img = style_img.to(self.device)\n",
    "            desired_content = desired_content.to(self.device)\n",
    "            style_content = style_content.to(self.device)\n",
    "            style_encodes = self.style_encoder(style_img)\n",
    "            content_encodes = self.content_encoder(desired_content)\n",
    "            \n",
    "            preds = self.model_G(content_encodes, style_encodes)\n",
    "            ocr_loss = self.ocr_loss(preds, desired_labels)\n",
    "            \n",
    "            style_label_encodes = self.content_encoder(style_labels)\n",
    "            \n",
    "            reconstructed = model_G(style_label_encodes, style_encodes)\n",
    "            reconstructed_loss = self.cons_loss(style_img, reconstructed)\n",
    "            \n",
    "            reconstructed_style_encode = self.style_encoder(reconstructed)\n",
    "            cycle = model_G(style_label_encodes, reconstructed_style_encode)\n",
    "            cycle_loss = self.cons_loss(style_img, cycle)\n",
    "            \n",
    "            ocr_loss_rec = self.ocr_loss(reconstructed, style_labels)\n",
    "            ocr_loss_total = (ocr_loss + ocr_loss_rec) / 2.\n",
    "            \n",
    "            perc_loss, tex_loss = self.perc_loss(style_img, preds)\n",
    "            enc_loss = 0 # self.typeface_loss(style_img, preds)\n",
    "            adv_loss = self.model_G_loss(reconstructed)\n",
    "            \n",
    "            loss = 0.07 * ocr_loss_total + 2.0 * cycle_loss + 2.0 * reconstructed_loss + 25.0 * perc_loss + 7.0 * tex_loss + 0 * enc_loss + 0.06 * adv_loss\n",
    "                \n",
    "    def run(self):\n",
    "        for epoch in range(self.total_epochs):\n",
    "            self.train()\n",
    "            with torch.no_grad():\n",
    "                self.validate(epoch)\n",
    "            if self.scheduler_G is not None:\n",
    "                self.scheduler_G.step()\n",
    "            if self.scheduler_D is not None:\n",
    "                self.scheduler_D.step()"
   ],
   "id": "ff8d4f70838fd74b",
   "outputs": [],
   "execution_count": 73
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-25T16:43:38.654995Z",
     "start_time": "2024-08-25T16:43:36.511335Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from stylegan import StyleBased_Generator\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "total_epochs = 500\n",
    "\n",
    "batch_size = 16\n",
    "train_dataloader = DataLoader(ImageDataset(train_dir, train_dir+'/words.json'), shuffle=True, batch_size=batch_size)\n",
    "val_dataloader = DataLoader(ImageDataset(val_dir, val_dir+'/words.json'), batch_size=batch_size)\n",
    "\n",
    "model_G = StyleBased_Generator(dim_latent=512)\n",
    "model_G.to(device)\n",
    "style_encoder = StyleEncoder().to(device)\n",
    "content_encoder = ContentEncoder().to(device)\n",
    "model_D = NLayerDiscriminator(input_nc=3, ndf=64, n_layers=3, norm_layer=(lambda x : torch.nn.Identity()))\n",
    "model_D.to(device)\n",
    "\n",
    "optimizer_G = torch.optim.AdamW(\n",
    "    list(model_G.parameters()) + \n",
    "    list(style_encoder.parameters()) +\n",
    "    list(style_encoder.parameters()),\n",
    "    lr=1e-3,\n",
    "    weight_decay=1e-6\n",
    ")\n",
    "scheduler_G = torch.optim.lr_scheduler.ExponentialLR(\n",
    "    optimizer_G,\n",
    "    gamma=0.9\n",
    ")\n",
    "optimizer_D = torch.optim.AdamW(model_D.parameters(), lr=1e-4)\n",
    "scheduler_D = torch.optim.lr_scheduler.ExponentialLR(\n",
    "    optimizer_D,\n",
    "    gamma=0.9\n",
    ")\n",
    "\n",
    "trainer = StyleGanTrainer(  \n",
    "    model_G,\n",
    "    model_D,\n",
    "    style_encoder,\n",
    "    content_encoder,\n",
    "    optimizer_G,\n",
    "    optimizer_D,\n",
    "    scheduler_G,\n",
    "    scheduler_D,\n",
    "    train_dataloader,\n",
    "    val_dataloader,\n",
    "    total_epochs=total_epochs,\n",
    "    ocr_loss=OCRLoss(),\n",
    "    perc_loss=VGGPerceptualLoss(),\n",
    "    cons_loss=torch.nn.L1Loss(),\n",
    "    adv_loss=torch.nn.MSELoss(),\n",
    "    device=device,\n",
    ")\n",
    "\n",
    "trainer.run()"
   ],
   "id": "67426edacbea3aac",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ryan Chin\\Documents\\Projects\\AI_dataset\\cropped\\train\n",
      "C:\\Users\\Ryan Chin\\Documents\\Projects\\AI_dataset\\cropped\\train\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ryan Chin\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\init.py:452: UserWarning: Initializing zero-element tensors is a no-op\n",
      "  warnings.warn(\"Initializing zero-element tensors is a no-op\")\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'ocr/TPS-ResNet-BiLSTM-Attn.pth'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mFileNotFoundError\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[74], line 46\u001B[0m\n\u001B[0;32m     28\u001B[0m optimizer_D \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39moptim\u001B[38;5;241m.\u001B[39mAdamW(model_D\u001B[38;5;241m.\u001B[39mparameters(), lr\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1e-4\u001B[39m)\n\u001B[0;32m     29\u001B[0m scheduler_D \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39moptim\u001B[38;5;241m.\u001B[39mlr_scheduler\u001B[38;5;241m.\u001B[39mExponentialLR(\n\u001B[0;32m     30\u001B[0m     optimizer_D,\n\u001B[0;32m     31\u001B[0m     gamma\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0.9\u001B[39m\n\u001B[0;32m     32\u001B[0m )\n\u001B[0;32m     34\u001B[0m trainer \u001B[38;5;241m=\u001B[39m StyleGanTrainer(  \n\u001B[0;32m     35\u001B[0m     model_G,\n\u001B[0;32m     36\u001B[0m     model_D,\n\u001B[0;32m     37\u001B[0m     style_encoder,\n\u001B[0;32m     38\u001B[0m     content_encoder,\n\u001B[0;32m     39\u001B[0m     optimizer_G,\n\u001B[0;32m     40\u001B[0m     optimizer_D,\n\u001B[0;32m     41\u001B[0m     scheduler_G,\n\u001B[0;32m     42\u001B[0m     scheduler_D,\n\u001B[0;32m     43\u001B[0m     train_dataloader,\n\u001B[0;32m     44\u001B[0m     val_dataloader,\n\u001B[0;32m     45\u001B[0m     total_epochs\u001B[38;5;241m=\u001B[39mtotal_epochs,\n\u001B[1;32m---> 46\u001B[0m     ocr_loss\u001B[38;5;241m=\u001B[39m\u001B[43mOCRLoss\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m,\n\u001B[0;32m     47\u001B[0m     perc_loss\u001B[38;5;241m=\u001B[39mVGGPerceptualLoss(),\n\u001B[0;32m     48\u001B[0m     cons_loss\u001B[38;5;241m=\u001B[39mtorch\u001B[38;5;241m.\u001B[39mnn\u001B[38;5;241m.\u001B[39mL1Loss(),\n\u001B[0;32m     49\u001B[0m     adv_loss\u001B[38;5;241m=\u001B[39mtorch\u001B[38;5;241m.\u001B[39mnn\u001B[38;5;241m.\u001B[39mMSELoss(),\n\u001B[0;32m     50\u001B[0m     device\u001B[38;5;241m=\u001B[39mdevice,\n\u001B[0;32m     51\u001B[0m )\n\u001B[0;32m     53\u001B[0m trainer\u001B[38;5;241m.\u001B[39mrun()\n",
      "Cell \u001B[1;32mIn[67], line 43\u001B[0m, in \u001B[0;36mOCRLoss.__init__\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m     40\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmodel \u001B[38;5;241m=\u001B[39m Model(opt)\n\u001B[0;32m     41\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmodel \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mnn\u001B[38;5;241m.\u001B[39mDataParallel(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmodel)\u001B[38;5;241m.\u001B[39mto(device)\n\u001B[1;32m---> 43\u001B[0m mappings \u001B[38;5;241m=\u001B[39m \u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mload\u001B[49m\u001B[43m(\u001B[49m\u001B[43mopt\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msaved_model\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmap_location\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdevice\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     44\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmodel\u001B[38;5;241m.\u001B[39mload_state_dict(mappings)\n\u001B[0;32m     45\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmodel\u001B[38;5;241m.\u001B[39meval()\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\serialization.py:998\u001B[0m, in \u001B[0;36mload\u001B[1;34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001B[0m\n\u001B[0;32m    995\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mencoding\u001B[39m\u001B[38;5;124m'\u001B[39m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;129;01min\u001B[39;00m pickle_load_args\u001B[38;5;241m.\u001B[39mkeys():\n\u001B[0;32m    996\u001B[0m     pickle_load_args[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mencoding\u001B[39m\u001B[38;5;124m'\u001B[39m] \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mutf-8\u001B[39m\u001B[38;5;124m'\u001B[39m\n\u001B[1;32m--> 998\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m \u001B[43m_open_file_like\u001B[49m\u001B[43m(\u001B[49m\u001B[43mf\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mrb\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m)\u001B[49m \u001B[38;5;28;01mas\u001B[39;00m opened_file:\n\u001B[0;32m    999\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m _is_zipfile(opened_file):\n\u001B[0;32m   1000\u001B[0m         \u001B[38;5;66;03m# The zipfile reader is going to advance the current file position.\u001B[39;00m\n\u001B[0;32m   1001\u001B[0m         \u001B[38;5;66;03m# If we want to actually tail call to torch.jit.load, we need to\u001B[39;00m\n\u001B[0;32m   1002\u001B[0m         \u001B[38;5;66;03m# reset back to the original position.\u001B[39;00m\n\u001B[0;32m   1003\u001B[0m         orig_position \u001B[38;5;241m=\u001B[39m opened_file\u001B[38;5;241m.\u001B[39mtell()\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\serialization.py:445\u001B[0m, in \u001B[0;36m_open_file_like\u001B[1;34m(name_or_buffer, mode)\u001B[0m\n\u001B[0;32m    443\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_open_file_like\u001B[39m(name_or_buffer, mode):\n\u001B[0;32m    444\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m _is_path(name_or_buffer):\n\u001B[1;32m--> 445\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43m_open_file\u001B[49m\u001B[43m(\u001B[49m\u001B[43mname_or_buffer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmode\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    446\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m    447\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mw\u001B[39m\u001B[38;5;124m'\u001B[39m \u001B[38;5;129;01min\u001B[39;00m mode:\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\serialization.py:426\u001B[0m, in \u001B[0;36m_open_file.__init__\u001B[1;34m(self, name, mode)\u001B[0m\n\u001B[0;32m    425\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__init__\u001B[39m(\u001B[38;5;28mself\u001B[39m, name, mode):\n\u001B[1;32m--> 426\u001B[0m     \u001B[38;5;28msuper\u001B[39m()\u001B[38;5;241m.\u001B[39m\u001B[38;5;21m__init__\u001B[39m(\u001B[38;5;28;43mopen\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mname\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmode\u001B[49m\u001B[43m)\u001B[49m)\n",
      "\u001B[1;31mFileNotFoundError\u001B[0m: [Errno 2] No such file or directory: 'ocr/TPS-ResNet-BiLSTM-Attn.pth'"
     ]
    }
   ],
   "execution_count": 74
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
